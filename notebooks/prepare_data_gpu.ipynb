{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from numba import jit, njit\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MKL_DEBUG_CPU_TYPE'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'data/dfdc-crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import ops\n",
    "\n",
    "import nvidia.dali as dali\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "# src\n",
    "sys.path.insert(0, SRC_DIR)\n",
    "from sample.reader import VideoReader\n",
    "from dataset.utils import read_labels\n",
    "\n",
    "# Pytorch_Retinaface\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'Pytorch_Retinaface'))\n",
    "from data import cfg_mnet\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from models.retinaface import RetinaFace\n",
    "from detect_utils import decode_batch, detect, load_model, postproc_detections\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_axis(c0, c1, pad, cmax):\n",
    "    c0 = max(0, c0 - pad)\n",
    "    c1 = min(cmax, c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "@njit\n",
    "def expand_bbox(bbox, pct):\n",
    "    bbox = np.copy(bbox)\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@njit\n",
    "def crop_face(img, bbox, pad_pct=0.05, square=True):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    \n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox(bbox, pad_pct)\n",
    "        \n",
    "    x0, y0, x1, y1 = bbox.astype(np.int16)\n",
    "    \n",
    "    if square:\n",
    "        w, h = x1 - x0, y1 - y0\n",
    "        if w > h:\n",
    "            pad = (w - h) // 2\n",
    "            y0, y1, h = calc_axis(y0, y1, pad, img_h)\n",
    "        elif h > w:\n",
    "            pad = (h - w) // 2\n",
    "            x0, x1, w = calc_axis(x0, x1, pad, img_w)\n",
    "    \n",
    "    size = min(w, h)\n",
    "    face = img[y0:y1, x0:x1][:size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_num_faces(num_faces, frac_thresh=0.25):\n",
    "    avg = num_faces.mean()\n",
    "    fraction, integral = np.modf(avg)\n",
    "    rounded = integral if fraction < frac_thresh else integral + 1\n",
    "    return int(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPipe(dali.pipeline.Pipeline):\n",
    "    def __init__(self, filenames: List[str], seq_len=30, stride=10, \n",
    "                 batch_size=1, num_threads=1, device_id=0):\n",
    "        super(VideoPipe, self).__init__(\n",
    "            batch_size, num_threads, device_id, seed=3)\n",
    "        self.input = dali.ops.VideoReader(\n",
    "            device='gpu', filenames=filenames, \n",
    "            sequence_length=seq_len,\n",
    "            shard_id=0, num_shards=1)\n",
    "\n",
    "    def define_graph(self):\n",
    "        output = self.input(name='reader')\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def get_file_list(df: pd.DataFrame, start: int, end: int, \n",
    "                  base_dir:str=DATA_DIR) -> List[str]:\n",
    "    path_fn = lambda row: os.path.join(base_dir, row.dir, row.name)\n",
    "    return df.iloc[start:end].apply(path_fn, axis=1).values.tolist()\n",
    "\n",
    "\n",
    "def build_data_iter(files: List[str]):\n",
    "    pipe = VideoPipe(files)\n",
    "    pipe.build()\n",
    "    return DALIGenericIterator([pipe], ['images'], len(files))\n",
    "\n",
    "\n",
    "def init_detector(cfg, weights, use_cpu=False):\n",
    "    cfg['pretrain'] = False\n",
    "    net = RetinaFace(cfg=cfg, phase='test')\n",
    "    net = load_model(net, weights, use_cpu)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def mkdirs(base_dir: str, chunk_dirs: List[str]) -> None:\n",
    "    for chunk_dir in chunk_dirs:\n",
    "        dir_path = os.path.join(base_dir, chunk_dir)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imgs(sample: np.ndarray) -> Tuple[Tensor, Tensor]:\n",
    "    n, h, w, c = sample.shape\n",
    "    \n",
    "    imgs = sample.float()\n",
    "    imgs -= torch.tensor([104, 117, 123], device=imgs.device)\n",
    "    imgs = imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    scale = torch.tensor([w, h, w, h])\n",
    "    return imgs, scale\n",
    "\n",
    "\n",
    "def detect(sample: Tensor, model, cfg: Dict[str,any], device: torch.device) -> Tensor:\n",
    "    bs = cfg['batch_size']\n",
    "    num_frames, height, width, ch = sample.shape\n",
    "    imgs, scale = prepare_imgs(sample)\n",
    "\n",
    "    priorbox = PriorBox(cfg, image_size=(height, width))\n",
    "    priors = priorbox.forward().to(device)\n",
    "    scale = scale.to(device)\n",
    "\n",
    "    detections = []\n",
    "    for start in range(0, num_frames, bs):\n",
    "        end = start + bs\n",
    "        imgs_batch = imgs[start:end] #.to(device)\n",
    "        with torch.no_grad():\n",
    "            loc, conf, landms = model(imgs_batch)\n",
    "        imgs_batch, landms = None, None\n",
    "        dets = postproc_detections(loc, conf, priors, scale, cfg)\n",
    "        detections.append(dets)\n",
    "        loc, conf = None, None\n",
    "    \n",
    "    return torch.cat(detections)\n",
    "\n",
    "\n",
    "def postproc_detections(\n",
    "        locations: Tensor, confidence: Tensor, priors: Tensor, scale: Tensor, \n",
    "        cfg: Dict[str,any], resize=1) -> Tensor:\n",
    "    boxes = decode_batch(locations, priors, cfg['variance'])\n",
    "    boxes = boxes * scale / resize\n",
    "    scores = confidence[:, :, 1]\n",
    "    num_frames = scores.shape[0]\n",
    "    dets = [postproc_frame_gpu(boxes[i], scores[i]) \n",
    "            for i in range(num_frames)]\n",
    "    return torch.cat(dets)\n",
    "\n",
    "\n",
    "def postproc_frame_gpu(\n",
    "        boxes: Tensor, scores: Tensor, score_thresh=0.75, \n",
    "        nms_thresh=0.4, top_k=500, keep_top_k=5) -> Tensor:\n",
    "    inds = (scores > score_thresh).nonzero()\n",
    "    if not inds.size(0):\n",
    "        return torch.empty(1, 0, 5, device=boxes.device, dtype=torch.float32)\n",
    "    else:\n",
    "        inds = inds[0]\n",
    "    boxes = boxes[inds]\n",
    "    scores = scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    scores, idxs = scores.sort(descending=True)\n",
    "    scores, idxs = scores[:top_k], idxs[:top_k]\n",
    "    boxes = boxes[idxs]\n",
    "\n",
    "    # do NMS\n",
    "    keep = torchvision.ops.nms(boxes, scores, nms_thresh)\n",
    "    boxes = boxes[keep][:keep_top_k]\n",
    "    scores = scores[keep][:keep_top_k]\n",
    "    \n",
    "    scores = scores.unsqueeze_(1)\n",
    "    return torch.cat([boxes, scores], dim=1).unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "        start=0, end=None, \n",
    "        num_frames_fake=30, num_frames_real=120,\n",
    "        use_cpu=False, bs=32, verbose=False,\n",
    "        base_dir=BASE_DIR, data_dir=DATA_DIR, save_dir=SAVE_DIR):\n",
    "    df = read_labels(data_dir)\n",
    "    mkdirs(save_dir, df['dir'].unique())\n",
    "    \n",
    "    reader = VideoReader()\n",
    "    device = torch.device(\"cpu\" if use_cpu else \"cuda\")\n",
    "    weights_mnet = os.path.join(base_dir, 'data/weights/mobilenet0.25_Final.pth')\n",
    "    cfg = {**cfg_mnet, 'batch_size': bs}\n",
    "    detector = init_detector(cfg, weights_mnet, use_cpu).to(device)\n",
    "    \n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "        \n",
    "    files = get_file_list(df, start, end)\n",
    "    data_iter = build_data_iter(files)\n",
    "        \n",
    "    for idx, batch in tqdm(enumerate(data_iter), total=(end-start)):\n",
    "        meta = df.iloc[idx]\n",
    "        # fake = bool(meta['label'])\n",
    "        \n",
    "        sample_dir = os.path.join(save_dir, meta.dir, meta.name[:-4])\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "        if verbose:\n",
    "            t0 = time.time()\n",
    "            \n",
    "        num_frames = 30 # num_frames_fake if fake else num_frames_real\n",
    "        \n",
    "        images = batch[0]['images'].squeeze(0)\n",
    "        detections = detect(images, detector, cfg_mnet, device)\n",
    "        detections = detections.cpu().numpy()\n",
    "        num_faces = np.array(list(map(len, detections)), dtype=np.uint8)\n",
    "        max_faces_per_frame = round_num_faces(num_faces, frac_thresh=0.25)\n",
    "        images = images.cpu().numpy()\n",
    "    \n",
    "        for f in range(num_frames):\n",
    "            for det in detections[f][:max_faces_per_frame]:\n",
    "                face = crop_face(images[f], det[:4])\n",
    "                file_path = os.path.join(sample_dir, '%03d.png' % f)\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "                # cv2.imwrite(file_path, face)\n",
    "        detections = None\n",
    "        # gc.collect()\n",
    "        \n",
    "        if verbose:\n",
    "            t1 = time.time()\n",
    "            print('[%6d][%.02f s] %s' % (idx, t1 - t0, sample_dir))\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/dmitry/projects/dfdc/data/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c732c2e00e84deda07ba9011a4201d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0][1.01 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/vkketnrfud\n",
      "[     1][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/tnsaqegyqt\n",
      "[     2][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/jcwkemycdm\n",
      "[     3][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/lnpsnoufkq\n",
      "[     4][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/rdfdbmyrqm\n",
      "[     5][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/otyrbsrkhn\n",
      "[     6][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/zmlpmfbryq\n",
      "[     7][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/sjhdwvfdbi\n",
      "[     8][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/npbreznxbl\n",
      "[     9][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/khoogmqdci\n",
      "[    10][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/zewjjvygcr\n",
      "[    11][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/qxfhktbwli\n",
      "[    12][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/yfihpubaxh\n",
      "[    13][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/fheautpznj\n",
      "[    14][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/vlyncoavxo\n",
      "[    15][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/wtykulolsd\n",
      "[    16][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/swljyvwykf\n",
      "[    17][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/ctusyabydj\n",
      "[    18][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/lnqttvbefa\n",
      "[    19][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/inwjmrkgid\n",
      "[    20][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/ekectxegkf\n",
      "[    21][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/jksqkdgclz\n",
      "[    22][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/xqxmeaetvc\n",
      "[    23][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/wuuvcuozwm\n",
      "[    24][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/kimxcwwkej\n",
      "[    25][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/tzcuzyosrc\n",
      "[    26][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/bdqffmwxqh\n",
      "[    27][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/xajzgrabnj\n",
      "[    28][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/lxvavsmyru\n",
      "[    29][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/oouvrspxmb\n",
      "[    30][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/rwqzraylnn\n",
      "[    31][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/jnfnwuvutq\n",
      "[    32][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/qfzxsyfdxg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 0 and 1 in dimension 1 at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/THC/generic/THCTensorMath.cu:71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b30a065e26b7>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(start, end, num_frames_fake, num_frames_real, use_cpu, bs, verbose, base_dir, data_dir, save_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg_mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mnum_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b8cdcbba5b26>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(sample, model, cfg, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimgs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostproc_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b8cdcbba5b26>\u001b[0m in \u001b[0;36mpostproc_detections\u001b[0;34m(locations, confidence, priors, scale, cfg, resize)\u001b[0m\n\u001b[1;32m     42\u001b[0m     dets = [postproc_frame_gpu(boxes[i], scores[i]) \n\u001b[1;32m     43\u001b[0m             for i in range(num_frames)]\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 0 and 1 in dimension 1 at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/THC/generic/THCTensorMath.cu:71"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# base - 1:06\n",
    "# gpu_nms - 1:04\n",
    "\n",
    "gc.collect()\n",
    "prepare_data(start=0, end=100, bs=30, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs224n] *",
   "language": "python",
   "name": "conda-env-cs224n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
