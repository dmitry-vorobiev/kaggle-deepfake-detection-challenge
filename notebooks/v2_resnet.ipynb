{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.3.1\n",
      "Torchvision Version:  0.4.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "\n",
    "FASTPART=False\n",
    "if FASTPART:\n",
    "    num_frames = 4\n",
    "else:\n",
    "    num_frames = 16\n",
    "    \n",
    "is_alchemy_used = True\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from skimage import io, transform\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Dataset\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "if is_alchemy_used:\n",
    "    from catalyst.dl import SupervisedAlchemyRunner as SupervisedRunner\n",
    "else:\n",
    "    from catalyst.dl import SupervisedRunner\n",
    "\n",
    "import random\n",
    "from scipy import ndimage\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from network.models import model_selection\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "from albumentations import Compose, RandomCrop, Normalize, HorizontalFlip, Resize, RandomResizedCrop, CenterCrop,PadIfNeeded\n",
    "from albumentations.pytorch import ToTensor\n",
    "from alchemy import Logger\n",
    "token = \"d1dd16f08d518293bcbeddd313b49aa4\"\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "def seed_everything(seed=12345):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.3.1, catalyst: 20.02.3\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List, Tuple \n",
    "\n",
    "import os\n",
    "import torch\n",
    "import catalyst\n",
    "\n",
    "from catalyst.dl import utils\n",
    "\n",
    "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "\n",
    "SEED = 42\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = f'/home/{os.environ[\"USER\"]}/projects/dfdc'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "HDF5_DIR = f'/home/{os.environ[\"USER\"]}/projects/dfdc/data/dfdc-crops/hdf5'\n",
    "IMG_DIR = f'/home/{os.environ[\"USER\"]}/projects/dfdc/data/dfdc-crops/webp'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 24#24#32\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 10\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "def my_initialize_model(file_checkpoint, model_name, feature_extract, emb_len):\n",
    "    \n",
    "\n",
    "    model, input_size = initialize_model(model_name, 2, feature_extract, use_pretrained=True)\n",
    "#     model = model.to(device)\n",
    "    if file_checkpoint != None:\n",
    "        print(f'Loading checkpoint {file_checkpoint}')\n",
    "        checkpoint = torch.load(file_checkpoint)#, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    _ = model.eval()\n",
    "    \n",
    "    if file_checkpoint != None:\n",
    "        del checkpoint\n",
    "\n",
    "#     emb_len = 128\n",
    "    if emb_len > 2:\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, emb_len)\n",
    "    return model, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filter(kernel_size = 7, sigma = 3, channels = 3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_cord = torch.arange(kernel_size)\n",
    "    x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1)\n",
    "\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    # the product of two gaussian distributions for two different\n",
    "    # variables (in this case called x and y)\n",
    "    gaussian_kernel = (1./(2.*math.pi*variance)) *\\\n",
    "                      torch.exp(\n",
    "                          -torch.sum((xy_grid - mean)**2., dim=-1) /\\\n",
    "                          (2*variance)\n",
    "                      )\n",
    "\n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "\n",
    "    # Reshape to 2d depthwise convolutional weight\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "    gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels, bias=False\n",
    "                                , padding=(int(kernel_size/2),int(kernel_size/2))\n",
    "                               )\n",
    "\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False\n",
    "    return gaussian_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_to_ij(num_frames, k):\n",
    "    num_in_row = int(np.sqrt(num_frames))\n",
    "    q = 0\n",
    "    for i in range(num_in_row):\n",
    "        for j in range(num_in_row):\n",
    "            if q == k:\n",
    "                return (i, j)\n",
    "            q += 1\n",
    "def ij_to_k(num_frames, i_in, j_in):\n",
    "    num_in_row = int(np.sqrt(num_frames))\n",
    "    q = 0\n",
    "    for i in range(num_in_row):\n",
    "        for j in range(num_in_row):\n",
    "            if i == i_in and j == j_in:\n",
    "                return q\n",
    "            q += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NetLstm(nn.Module):\n",
    "    def __init__(self, checkpoint_file, model_name, emb_len, hidden_dim):\n",
    "        super(NetLstm, self).__init__()\n",
    "        self.backbone, self.input_size = my_initialize_model(checkpoint_file, model_name, False, emb_len)\n",
    "        self.lstm = nn.LSTM(emb_len, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, 2)\n",
    "#         self.out2tag = nn.Linear(self.input_size, 2)\n",
    "        self.filter = create_filter(kernel_size = 7, sigma = 3, channels = 3)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        self.lstm.flatten_parameters()\n",
    "        \n",
    "        tag_scores_list = torch.zeros((sentences.shape[0], 2), dtype=torch.float32 ).cuda()\n",
    "#         print(tag_scores_list.shape)\n",
    "        for i, sentence in enumerate(sentences):   \n",
    "  \n",
    "            sentence = sentence.permute(0, 3, 1, 2)\n",
    "            embeds = self.backbone(sentence - self.filter(sentence))\n",
    "#             print(embeds.shape)\n",
    "            lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "#             print(lstm_out.shape)\n",
    "            tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))     \n",
    "#             print(tag_space.shape)\n",
    "            tag_scores_list[i] = tag_space[-1,:]\n",
    "        return tag_scores_list\n",
    "    \n",
    "class NetRes(nn.Module):\n",
    "    def __init__(self, checkpoint_file, model_name):\n",
    "        super(NetRes, self).__init__()\n",
    "        self.backbone, self.input_size = my_initialize_model(checkpoint_file, model_name, False, 2)\n",
    " \n",
    "        self.filter = create_filter(kernel_size = 7, sigma = 3, channels = 3)\n",
    "        self.norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \n",
    "        tag_scores_list = torch.zeros((sentences.shape[0], 2), dtype=torch.float32 ).cuda()\n",
    "#         print(tag_scores_list.shape)\n",
    "        for i, sentence in enumerate(sentences):  \n",
    "                    \n",
    "            sentence = sentence.permute(0, 3, 1, 2)\n",
    "            sentence = sentence - self.filter(sentence)\n",
    "            for j in range(sentence.shape[0]):\n",
    "                sentence[j] = self.norm(sentence[j])\n",
    "            embeds = self.backbone(sentence)\n",
    "#             print(embeds.shape)\n",
    "            tag_scores_list[i] = embeds.mean(axis=0)\n",
    "        return tag_scores_list\n",
    "\n",
    "class NetResThr(nn.Module):\n",
    "    def __init__(self, checkpoint_file, model_name, emb_len, num_frames=4 ):\n",
    "        super(NetResThr, self).__init__()\n",
    "        self.backbone, self.input_size = my_initialize_model(checkpoint_file, model_name, False, emb_len)\n",
    "        self.emb_len = emb_len\n",
    "        self.num_frames = num_frames\n",
    "        self.img_in_row = int(np.sqrt(self.num_frames))\n",
    "        self.sz_in_row = int(self.input_size/self.img_in_row)\n",
    "#         self.filter = create_filter(kernel_size = 7, sigma = 3, channels = 3)\n",
    "        self.norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        self.fc =   nn.Linear(self.num_frames * emb_len, 2)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \n",
    "        tag_scores_list = torch.zeros((sentences.shape[0], 2), dtype=torch.float32 ).cuda()\n",
    "        \n",
    "        for k, sentence in enumerate(sentences): \n",
    "            \n",
    "            sentence = sentence.permute(0, 3, 1, 2)\n",
    "            sentence_generated = torch.zeros(sentence.shape, dtype=torch.float32).cuda()\n",
    "\n",
    "            for frame_out in range(num_frames):\n",
    "                for frame_in in range(num_frames):\n",
    "                    for pt_out in range(num_frames):            \n",
    "\n",
    "                        i_in, j_in = k_to_ij(self.num_frames, frame_out)\n",
    "                        i_out, j_out = k_to_ij(self.num_frames, pt_out)\n",
    "                        sentence_generated[frame_out,:,i_out*self.sz_in_row:(i_out+1)*self.sz_in_row, j_out*self.sz_in_row:(j_out+1)*self.sz_in_row] = \\\n",
    "                            sentence[frame_in, :, i_in*self.sz_in_row:(i_in+1)*self.sz_in_row, j_in*self.sz_in_row:(j_in+1)*self.sz_in_row]                    \n",
    "                sentence_generated[frame_out] = self.norm(sentence_generated[frame_out] )\n",
    "            embeds = self.backbone(sentence_generated)\n",
    "            embeds = torch.flatten(embeds)\n",
    "            embeds = self.fc( embeds )\n",
    "            tag_scores_list[k] = embeds#.mean(axis=0)\n",
    "        return tag_scores_list    \n",
    "\n",
    "# model = NetLstm('/home/kb/Documents/best0.pth', 'resnet', 16, 16)\n",
    "# model = NetLstm(None, 'resnet', 4, 4)\n",
    "# model = NetRes(None, 'resnet')\n",
    "emb_len = 32\n",
    "model = NetResThr(None, 'resnet', emb_len, num_frames)\n",
    "input_size = model.input_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Iterator, List, Optional, Tuple, Union\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(BASE_DIR, 'src'))\n",
    "from dataset.utils import read_labels\n",
    "from prepare_data import get_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len_hdf5(path):\n",
    "    lens = dict()\n",
    "    for name in os.listdir(path):\n",
    "        full_path = os.path.join(path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            with h5py.File(full_path, 'r+') as f:\n",
    "                lens[name] = len(f)\n",
    "    return lens\n",
    "\n",
    "\n",
    "def check_len_images(path):\n",
    "    lens = dict()\n",
    "    for name in os.listdir(path):\n",
    "        full_path = os.path.join(path, name)\n",
    "        if os.path.isdir(full_path):\n",
    "            lens[name] = len(os.listdir(full_path))\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_frames(n: int, total: int) -> np.ndarray:\n",
    "    idxs = np.linspace(0, total, min(n, total), dtype=int, endpoint=False)\n",
    "    rnd_shift = np.random.randint(0, (total - idxs[-1]))\n",
    "    return idxs + rnd_shift\n",
    "\n",
    "\n",
    "def rnd_slice_frames(n: int, total: int, stride=1.) -> np.ndarray:\n",
    "    idxs = np.arange(0, total, stride)[:n].astype(np.uint16)\n",
    "    rnd_shift = np.random.randint(0, (total - idxs[-1]))\n",
    "    return idxs + rnd_shift\n",
    "\n",
    "\n",
    "def create_mask(idxs: np.ndarray, total: int) -> np.ndarray:\n",
    "    mask = np.zeros(total, dtype=np.bool)\n",
    "    mask[idxs] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def pad(frames: np.ndarray, amount: int, where :str='start') -> np.ndarray:\n",
    "    dims = np.zeros((frames.ndim, 2), dtype=np.int8)\n",
    "    pad_dim = 1 if where == 'end' else 0\n",
    "    dims[0, pad_dim] = amount\n",
    "    return np.pad(frames, dims, 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSampler():\n",
    "    def __init__(self, num_frames: int, real_fake_ratio: float, \n",
    "                 p_sparse: float):\n",
    "        self.num_frames = num_frames\n",
    "        self.real_fake_ratio = real_fake_ratio\n",
    "        self.p_sparse = p_sparse\n",
    "        \n",
    "    def __call__(self, label: Tuple[int, bool]) -> Callable[[int], np.ndarray]:\n",
    "        dice = np.random.rand()\n",
    "        if dice < self.p_sparse:\n",
    "            return partial(sparse_frames, self.num_frames)\n",
    "        else:\n",
    "            # Stored frames: fake - 30, real - 150, \n",
    "            # the real_fake_ratio should be set to 150 / 30 = 5\n",
    "            # stride for fake: 5 - (4 * 1) = 1\n",
    "            # stride for real: 5 - (4 * 0) = 5\n",
    "            n = self.real_fake_ratio\n",
    "            stride = n - ((n-1) * int(label))\n",
    "            return partial(rnd_slice_frames, self.num_frames, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = FrameSampler(num_frames=15, real_fake_ratio=100/30, p_sparse=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_path: str, size: Tuple[int, int], \n",
    "                 sampler: FrameSampler, \n",
    "                 sub_dirs: Optional[List[str]]=None):\n",
    "        self.base_path = base_path\n",
    "        self.size = size\n",
    "        self.sampler = sampler\n",
    "        self.df = ImagesDataset._read_annotations(base_path, sub_dirs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _read_annotations(base_path: str, \n",
    "                          sub_dirs: Optional[List[str]]) -> pd.DataFrame:\n",
    "        if not os.path.isdir(base_path):\n",
    "            raise RuntimeError('Unable to access %s' % base_path)\n",
    "        parts = []\n",
    "        load_all = sub_dirs is None\n",
    "        if load_all:\n",
    "            sub_dirs = os.listdir(base_path)\n",
    "        for chunk_dir in sub_dirs:\n",
    "            chunk_path = Path(base_path)/chunk_dir\n",
    "            if not chunk_path.is_dir():\n",
    "                if not load_all:\n",
    "                    print('Invalid dir: %s' % str(chunk_path))\n",
    "                continue\n",
    "            files = os.listdir(chunk_path)\n",
    "            df = pd.DataFrame(files, columns=['video'])\n",
    "            df['label'] = df['video'].str.endswith('_1')\n",
    "            df['dir'] = chunk_dir\n",
    "            parts.append(df)\n",
    "        if len(parts) < 1:\n",
    "            raise AttributeError('No images were found')\n",
    "        return pd.concat(parts).reset_index()\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_image_folder(path: str, num_frames: int, size: int,\n",
    "                          sample_fn: Callable[[int], np.ndarray]) -> np.ndarray:\n",
    "        img_size = (size, size)\n",
    "        images = []\n",
    "        files = sorted(os.listdir(path))\n",
    "        total_frames = len(files)\n",
    "        if total_frames > 0:\n",
    "            idxs = sample_fn(total_frames)\n",
    "            pick = create_mask(idxs, total_frames)\n",
    "            for i, file in enumerate(files):\n",
    "                if pick[i]:\n",
    "                    img_path = os.path.join(path, file)\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    if img.shape[0] > input_size:\n",
    "                        img = img[int(img.shape[0]/2)-int(input_size/2):int(img.shape[0]/2)+int(input_size/2),:,:]\n",
    "                    if img.shape[1] > input_size:\n",
    "                        img = img[:, int(img.shape[1]/2)-int(input_size/2):int(img.shape[1]/2)+int(input_size/2),:]\n",
    "                    img = PadIfNeeded(min_height=input_size, min_width=input_size)(image=img)['image']\n",
    "                    \n",
    "#                     img = cv2.resize(img, img_size, \n",
    "#                                      interpolation=cv2.INTER_NEAREST)\n",
    "                    images.append(img)\n",
    "            return np.stack(images)\n",
    "        else:\n",
    "            return np.empty((0, size, size, 3), dtype=np.uint8)\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[np.ndarray, int]:\n",
    "        num_frames, size = self.size\n",
    "        meta = self.df.iloc[idx]\n",
    "        label = int(meta.label)\n",
    "        path = os.path.join(self.base_path, meta.dir, meta.video)\n",
    "        \n",
    "        if os.path.isdir(path):\n",
    "            sample_fn = self.sampler(meta.label)\n",
    "            frames = ImagesDataset.read_image_folder(\n",
    "                path, num_frames, size, sample_fn=sample_fn)\n",
    "        else:\n",
    "            print('Dir not found: {}'.format(path))\n",
    "            frames = np.zeros((num_frames, size, size, 3), dtype=np.uint8)\n",
    "        \n",
    "        if len(frames) > 0:\n",
    "            pad_amount = num_frames - len(frames)\n",
    "            if pad_amount > 0:\n",
    "                frames = pad(frames, pad_amount, 'start')\n",
    "        else:\n",
    "            print('Empty file {}'.format(path))\n",
    "            frames = np.zeros((num_frames, size, size, 3), dtype=np.uint8)\n",
    "            \n",
    "        frames = np.array(frames, dtype=np.float32)\n",
    "        tr = Compose([\n",
    "            \n",
    "            CenterCrop(170, 80),\n",
    "            Resize(input_size, input_size, interpolation=3, p=1),\n",
    "#             RandomResizedCrop(input_size, input_size, scale=(0.2, 0.3), \n",
    "#                                          ratio=(0.8, 1.2), \n",
    "#                                          interpolation=3, always_apply=True, p=1.0),\n",
    "            \n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#             ToTensor()\n",
    "\n",
    "        ])\n",
    "        frames =np.asarray([tr(image=frame)['image'] for frame in frames ], dtype=np.float32)\n",
    "\n",
    "#         print(frames.shape)\n",
    "#         kernel = np.array([[-1, -1, -1],\n",
    "#                    [-1,  8, -1],\n",
    "#                    [-1, -1, -1]])\n",
    "#         for i in range(frames.shape[0]):\n",
    "#             for j in range(3):\n",
    "#                 frames[i,:,:,j] = ndimage.convolve(frames[i,:,:,j], kernel)\n",
    "            \n",
    "        \n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffled_idxs(values: np.ndarray, val: int) -> List[int]:\n",
    "    idxs = (values == val).nonzero()[0]\n",
    "    idxs = np.random.permutation(idxs)\n",
    "    return idxs\n",
    "\n",
    "\n",
    "class BalancedSampler(torch.utils.data.RandomSampler):\n",
    "    def __init__(self, data_source, replacement=False, num_samples=None):\n",
    "        \n",
    "        super().__init__(data_source, replacement, num_samples)\n",
    "        if not hasattr(data_source, 'df'):\n",
    "            raise ValueError(\"DataSource must have a 'df' property\")\n",
    "            \n",
    "        if not 'label' in data_source.df: \n",
    "            raise ValueError(\"DataSource.df must have a 'label' column\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        df = self.data_source.df\n",
    "        all_labels = df['label'].values\n",
    "        uniq_labels, label_freq = np.unique(all_labels, return_counts=True)\n",
    "        rev_freq = (len(all_labels) / label_freq)\n",
    "        \n",
    "        idxs = []\n",
    "        for freq, label in zip(rev_freq, uniq_labels):\n",
    "            fraction, times = np.modf(freq)\n",
    "            label_idxs = (all_labels == label).nonzero()[0]\n",
    "            for _ in range(int(times)):\n",
    "                label_idxs = np.random.permutation(label_idxs)\n",
    "                idxs.append(label_idxs)\n",
    "            if fraction > 0.05:\n",
    "                label_idxs = np.random.permutation(label_idxs)\n",
    "                chunk = int(len(label_idxs) * fraction)\n",
    "                idxs.append(label_idxs[:chunk])\n",
    "        idxs = np.concatenate(idxs)\n",
    "        idxs = np.random.permutation(idxs)[:self.num_samples]\n",
    "        return iter(idxs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19114\n",
      "Invalid dir: /home/kb/projects/dfdc/data/dfdc-crops/webp/dfdc_train_part_42\n",
      "Invalid dir: /home/kb/projects/dfdc/data/dfdc-crops/webp/dfdc_train_part_43\n",
      "Invalid dir: /home/kb/projects/dfdc/data/dfdc-crops/webp/dfdc_train_part_44\n",
      "Invalid dir: /home/kb/projects/dfdc/data/dfdc-crops/webp/dfdc_train_part_47\n",
      "Invalid dir: /home/kb/projects/dfdc/data/dfdc-crops/webp/dfdc_train_part_48\n",
      "Invalid dir: /home/kb/projects/dfdc/data/dfdc-crops/webp/dfdc_train_part_49\n",
      "8394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_loader(num_frames=15, real_fake_ratio=1, p_sparse=0.5, input_size=input_size, img_dir=None, sub_dirs=None):\n",
    "    \n",
    "    sampler = FrameSampler(num_frames, real_fake_ratio=real_fake_ratio, p_sparse=p_sparse)\n",
    "    ds = ImagesDataset(img_dir, size=(num_frames, input_size), sampler=sampler,\n",
    "                       sub_dirs =sub_dirs)\n",
    "    print(len(ds))\n",
    "    s = BalancedSampler(ds)\n",
    "    batch_sampler = torch.utils.data.BatchSampler(\n",
    "        BalancedSampler(ds), \n",
    "        batch_size=batch_size, \n",
    "        drop_last=True\n",
    "        \n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_sampler=batch_sampler)\n",
    "    return dl\n",
    "    \n",
    "loaders = {}\n",
    "loaders['train'] = get_loader(num_frames=num_frames, real_fake_ratio=100/30, p_sparse=1.0, input_size=input_size, \n",
    "                              img_dir='/home/kb/projects/dfdc/data/dfdc-crops/webp',\n",
    "                              sub_dirs= ['dfdc_train_part_%d' % i for i in [1,5,10,15,20,25,30,35]]\n",
    "                             )\n",
    "loaders['valid'] = get_loader(num_frames=num_frames, real_fake_ratio=100/30, p_sparse=1.0, input_size=input_size, \n",
    "                              img_dir='/home/kb/projects/dfdc/data/dfdc-crops/webp',\n",
    "                              sub_dirs= ['dfdc_train_part_%d' % i for i in range(40,50)]\n",
    "                             )\n",
    "# loaders['test'] = get_loader(num_frames=num_frames, real_fake_ratio=100/30, p_sparse=1.0, input_size=input_size, \n",
    "#                               img_dir='/home/kb/projects/dfdc/data/dfdc-crops/webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Experiment: exp0\n",
      "1/25 * Epoch (train): 100% 796/796 [51:56<00:00,  3.92s/it, loss=0.507]\n",
      "1/25 * Epoch (valid): 100% 349/349 [20:36<00:00,  3.54s/it, loss=0.684]\n",
      "[2020-03-09 12:27:47,754] \n",
      "1/25 * Epoch 1 (train): _base/lr=1.000e-05 | _base/momentum=0.9000 | _timers/_fps=6.7718 | _timers/batch_time=3.5455 | _timers/data_time=0.9787 | _timers/model_time=2.5668 | loss=0.6262\n",
      "1/25 * Epoch 1 (valid): _base/lr=1.000e-05 | _base/momentum=0.9000 | _timers/_fps=6.7853 | _timers/batch_time=3.5381 | _timers/data_time=1.0160 | _timers/model_time=2.5220 | loss=0.6964\n",
      "2/25 * Epoch (train): 100% 796/796 [52:19<00:00,  3.94s/it, loss=0.434]\n",
      "2/25 * Epoch (valid):  20% 69/349 [04:08<16:36,  3.56s/it, loss=0.669]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "project = 'dfdc_v2_resnet'\n",
    "num_epochs = 25\n",
    "\n",
    "group = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "\n",
    "if FASTPART:\n",
    "    group = f'fast_{group}'\n",
    "    \n",
    "expnum = 0\n",
    "experiment = f\"exp{expnum}\"\n",
    "logdir = f\"/home/kb/hdd/logs/deepfake/{project}/{group}/{experiment}\"\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "else:\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            pass\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# model runner\n",
    "runner = SupervisedRunner()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'----------------Experiment: {experiment}')\n",
    "logger = Logger(\n",
    "    token=token,\n",
    "    experiment=experiment,\n",
    "    group=group,\n",
    "    project=project,\n",
    ")\n",
    "\n",
    "logger.close()\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    monitoring_params={\n",
    "        \"token\": token,\n",
    "        \"project\": project,\n",
    "        \"experiment\": experiment,\n",
    "        \"group\": group,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_frames = num_frames\n",
    "# img_in_row = int(np.sqrt(num_frames))\n",
    "# sz_in_row = int(input_size/img_in_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for sentences, labels in loaders['train']:\n",
    "    \n",
    "#     for k, sentence in enumerate(sentences): \n",
    "#         print(f'showing {k}-th video')\n",
    "#         sentence = sentence.permute(0, 3, 1, 2)\n",
    "#         sentence_generated = torch.zeros(sentence.shape, dtype=torch.float32).cuda()\n",
    "        \n",
    "#         for frame_out in range(num_frames):\n",
    "#             for frame_in in range(num_frames):\n",
    "#                 for pt_out in range(num_frames):            \n",
    "                \n",
    "#                     i_in, j_in = k_to_ij(num_frames, frame_out)\n",
    "#                     i_out, j_out = k_to_ij(num_frames, pt_out)\n",
    "#                     sentence_generated[frame_out,:,i_out*sz_in_row:(i_out+1)*sz_in_row, j_out*sz_in_row:(j_out+1)*sz_in_row] = \\\n",
    "#                         sentence[frame_in, :, i_in*sz_in_row:(i_in+1)*sz_in_row, j_in*sz_in_row:(j_in+1)*sz_in_row]\n",
    "\n",
    "                \n",
    "      \n",
    "    \n",
    "    \n",
    "#         for j in range(sentence_generated.shape[0]):\n",
    "#             print(f'---showing {j}-th frame')\n",
    "            \n",
    "#             plt.figure()\n",
    "#             img = sentence_generated[j,:,:].permute(1,2,0).cpu().numpy() \n",
    "#             img -= img.min()\n",
    "#             img /= img.max() / 255.\n",
    "#             img = np.array(img, dtype=np.uint8)\n",
    "#             print(f'max {img[j,:,:].max()}, min {img[j,:,:].min()}')\n",
    "#             plt.imshow(  img)\n",
    "#             plt.show()\n",
    "        \n",
    "    \n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_generated[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence[0].min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df_env",
   "language": "python",
   "name": "df_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
