{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.3.1, ignite: 0.3.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import datetime as dt\n",
    "import ignite\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "from torchvision import transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Callable, Dict, Iterable, List, Tuple\n",
    "\n",
    "print(f\"torch: {torch.__version__}, ignite: {ignite.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "HDF5_DIR = '/media/dmitry/other/dfdc-crops/hdf5'\n",
    "IMG_DIR = '/media/dmitry/other/dfdc-crops/webp_lossy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'vendors/Pytorch_Retinaface'))\n",
    "sys.path.insert(0, SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, './utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.hdf5 import HDF5Dataset\n",
    "from dataset.images import ImagesDataset\n",
    "from dataset.sample import FrameSampler, BalancedSampler\n",
    "from model.detector import basic_detector_256, DetectorOut\n",
    "from model.loss import combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualise import show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = Tuple[FloatTensor, LongTensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(bs: int, num_frames: int, real_fake_ratio: float, \n",
    "                      p_sparse_frames: float, chunks: Iterable[int]\n",
    "                     ) -> torch.utils.data.DataLoader:\n",
    "    dirs = [f'dfdc_train_part_{i}' for i in chunks]\n",
    "    \n",
    "    sampler = FrameSampler(num_frames, \n",
    "                           real_fake_ratio=real_fake_ratio, \n",
    "                           p_sparse=p_sparse_frames)\n",
    "    tfms = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    ds = HDF5Dataset(HDF5_DIR, \n",
    "                     size=(num_frames, 256), \n",
    "                     sampler=sampler, \n",
    "                     transforms=tfms, \n",
    "                     sub_dirs=dirs)\n",
    "    print('Num samples: {}'.format(len(ds)))\n",
    "    \n",
    "    batch_sampler = torch.utils.data.BatchSampler(\n",
    "        BalancedSampler(ds),\n",
    "        batch_size=bs, \n",
    "        drop_last=True\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_sampler=batch_sampler)\n",
    "    return dl\n",
    "\n",
    "\n",
    "def prepare_batch(batch: Batch) -> Batch:\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 61779\n"
     ]
    }
   ],
   "source": [
    "train_dl = create_dataloader(\n",
    "    bs=12, \n",
    "    num_frames=10, \n",
    "    real_fake_ratio=100/30, \n",
    "    p_sparse_frames=0.75, \n",
    "    chunks=range(5,30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 7937\n"
     ]
    }
   ],
   "source": [
    "valid_dl = create_dataloader(\n",
    "    bs=12, \n",
    "    num_frames=10, \n",
    "    real_fake_ratio=100/30, \n",
    "    p_sparse_frames=1., \n",
    "    chunks=range(0,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Conv3D pooling: 2 layers\n"
     ]
    }
   ],
   "source": [
    "model = basic_detector_256()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(12, 3, 10, 256, 256, device=device)\n",
    "y = torch.randint(0, 2, (12,), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe2e991d061423b9302356ced00923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 29s, sys: 6.11 s, total: 2min 36s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = iter(train_dl)\n",
    "data = list(map(lambda _: next(data), tqdm(range(100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9227dfc1951047fcadf5db21f2214791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.14 s, sys: 1.72 s, total: 7.86 s\n",
      "Wall time: 7.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for batch in tqdm(data):\n",
    "    batch = prepare_batch(batch)\n",
    "    out = model(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 - 7.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[9], gamma=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_outs(batch: Batch, model_out: DetectorOut, \n",
    "                loss: FloatTensor) -> Dict[str, Tensor]:\n",
    "    out = {'loss': loss.item(),\n",
    "           'y_pred': (model_out[-1] >= 0.5).float(),\n",
    "           'y_true': batch[-1]}\n",
    "    return out\n",
    "\n",
    "\n",
    "def train(trainer: Engine, batch: Batch) -> int:\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    x, y = prepare_batch(batch)\n",
    "    out = model(x, y)\n",
    "    loss = combined_loss(out, x, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return gather_outs(batch, out, loss)\n",
    "\n",
    "\n",
    "def validate(trainer: Engine, batch: Batch) -> int:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = prepare_batch(batch)\n",
    "        out = model(x, y)\n",
    "        loss = combined_loss(out, x, y)\n",
    "    return gather_outs(batch, out, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Engine(train)\n",
    "evaluator = Engine(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy(output_transform=lambda out: [out['y_pred'], out['y_true']])\n",
    "accuracy.attach(trainer,   'acc')\n",
    "accuracy.attach(evaluator, 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_STARTED)\n",
    "@evaluator.on(Events.EPOCH_STARTED)\n",
    "def start_epoch(engine: Engine):\n",
    "    t0 = time.time()\n",
    "    trainer.state.ep_time = t0\n",
    "    trainer.state.time = t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_iter(engine: Engine, title: str) -> None:\n",
    "    epoch = trainer.state.epoch\n",
    "    iteration = engine.state.iteration\n",
    "    loss = engine.state.output['loss']\n",
    "    t_ep = trainer.state.ep_time\n",
    "    t0 = trainer.state.time\n",
    "    t1 = time.time()\n",
    "    cur_time = dt.datetime.fromtimestamp(t1).strftime('%H:%M:%S')\n",
    "    print(\"[{}][{:.2f} s] {:>5} | ep: {:3d}, it: {:5d}, loss: {:.5f}\".format(\n",
    "        cur_time, t1 - t0, title, epoch, iteration, loss))\n",
    "    trainer.state.time = t1\n",
    "    \n",
    "    \n",
    "def log_epoch(engine: Engine, title: str) -> None:\n",
    "    epoch = trainer.state.epoch\n",
    "    metrics = engine.state.metrics\n",
    "    print(\"{:>5} | ep: {}, acc: {:.3f}\\n\".format(\n",
    "        title, epoch, metrics['acc']))\n",
    "\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=5))\n",
    "def log_train_iter(engine: Engine) -> None:\n",
    "    log_iter(engine, 'train')\n",
    "    \n",
    "    \n",
    "@evaluator.on(Events.ITERATION_COMPLETED(every=5))\n",
    "def log_val_iter(engine: Engine) -> None:\n",
    "    log_iter(engine, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def evaluate(trainer):\n",
    "    with evaluator.add_event_handler(Events.COMPLETED, log_epoch, 'val'):\n",
    "        evaluator.run(valid_dl, epoch_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:36:19][2.27 s] train | ep:   1, it:     5, loss: 1.72960\n",
      "[00:36:21][2.35 s] train | ep:   1, it:    10, loss: 1.94522\n",
      "[00:36:24][3.12 s] train | ep:   1, it:    15, loss: 1.69148\n",
      "[00:36:28][3.25 s] train | ep:   1, it:    20, loss: 1.71001\n",
      "[00:36:31][3.24 s] train | ep:   1, it:    25, loss: 1.73655\n",
      "[00:36:34][3.43 s] train | ep:   1, it:    30, loss: 1.72883\n",
      "[00:36:38][4.27 s] train | ep:   1, it:    35, loss: 1.70893\n",
      "[00:36:43][4.46 s] train | ep:   1, it:    40, loss: 1.70642\n",
      "[00:36:48][4.62 s] train | ep:   1, it:    45, loss: 1.72912\n",
      "[00:36:52][4.67 s] train | ep:   1, it:    50, loss: 1.70249\n",
      "[00:36:57][4.79 s] train | ep:   1, it:    55, loss: 1.63429\n",
      "[00:37:02][4.52 s] train | ep:   1, it:    60, loss: 1.67645\n",
      "[00:37:06][4.51 s] train | ep:   1, it:    65, loss: 1.72357\n",
      "[00:37:10][4.24 s] train | ep:   1, it:    70, loss: 1.66377\n",
      "[00:37:15][4.32 s] train | ep:   1, it:    75, loss: 1.70160\n",
      "[00:37:19][4.55 s] train | ep:   1, it:    80, loss: 1.82731\n",
      "[00:37:23][4.28 s] train | ep:   1, it:    85, loss: 1.70622\n",
      "[00:37:28][4.33 s] train | ep:   1, it:    90, loss: 1.67634\n",
      "[00:37:32][4.60 s] train | ep:   1, it:    95, loss: 1.75483\n",
      "[00:37:37][4.61 s] train | ep:   1, it:   100, loss: 1.67447\n",
      "[00:37:38][1.33 s]   val | ep:   1, it:     5, loss: 1.68024\n",
      "  val | ep: 1, acc: 0.433\n",
      "\n",
      "[00:37:43][4.41 s] train | ep:   2, it:   105, loss: 1.72288\n",
      "[00:37:47][4.40 s] train | ep:   2, it:   110, loss: 1.66851\n",
      "[00:37:52][4.55 s] train | ep:   2, it:   115, loss: 1.72220\n",
      "[00:37:56][4.39 s] train | ep:   2, it:   120, loss: 1.72965\n",
      "[00:38:00][4.15 s] train | ep:   2, it:   125, loss: 1.64300\n",
      "[00:38:05][4.31 s] train | ep:   2, it:   130, loss: 1.69527\n",
      "[00:38:09][4.28 s] train | ep:   2, it:   135, loss: 1.68823\n",
      "[00:38:13][4.25 s] train | ep:   2, it:   140, loss: 1.66025\n",
      "[00:38:17][4.20 s] train | ep:   2, it:   145, loss: 1.65197\n",
      "[00:38:22][4.76 s] train | ep:   2, it:   150, loss: 1.69513\n",
      "[00:38:27][4.72 s] train | ep:   2, it:   155, loss: 1.71128\n",
      "[00:38:31][4.34 s] train | ep:   2, it:   160, loss: 1.68467\n",
      "[00:38:35][4.20 s] train | ep:   2, it:   165, loss: 1.73899\n",
      "[00:38:40][4.48 s] train | ep:   2, it:   170, loss: 1.70963\n",
      "[00:38:44][4.67 s] train | ep:   2, it:   175, loss: 1.70241\n",
      "[00:38:49][4.60 s] train | ep:   2, it:   180, loss: 1.71116\n",
      "[00:38:53][4.45 s] train | ep:   2, it:   185, loss: 1.69353\n",
      "[00:38:58][4.37 s] train | ep:   2, it:   190, loss: 1.71770\n",
      "[00:39:02][4.35 s] train | ep:   2, it:   195, loss: 1.70761\n",
      "[00:39:07][4.80 s] train | ep:   2, it:   200, loss: 1.72252\n",
      "[00:39:08][1.36 s]   val | ep:   2, it:     5, loss: 1.70763\n",
      "  val | ep: 2, acc: 0.400\n",
      "\n",
      "[00:39:13][4.43 s] train | ep:   3, it:   205, loss: 1.71065\n",
      "[00:39:17][4.29 s] train | ep:   3, it:   210, loss: 1.72564\n",
      "[00:39:22][4.46 s] train | ep:   3, it:   215, loss: 1.67828\n",
      "[00:39:26][4.46 s] train | ep:   3, it:   220, loss: 1.71846\n",
      "[00:39:30][4.35 s] train | ep:   3, it:   225, loss: 1.72033\n",
      "[00:39:35][4.35 s] train | ep:   3, it:   230, loss: 1.72047\n",
      "[00:39:39][4.60 s] train | ep:   3, it:   235, loss: 1.71611\n",
      "[00:39:43][3.90 s] train | ep:   3, it:   240, loss: 1.69633\n",
      "[00:39:48][4.56 s] train | ep:   3, it:   245, loss: 1.68754\n",
      "[00:39:52][4.68 s] train | ep:   3, it:   250, loss: 1.72205\n",
      "[00:39:57][4.30 s] train | ep:   3, it:   255, loss: 1.69417\n",
      "[00:40:01][4.22 s] train | ep:   3, it:   260, loss: 1.70693\n",
      "[00:40:06][4.67 s] train | ep:   3, it:   265, loss: 1.70947\n",
      "[00:40:10][4.54 s] train | ep:   3, it:   270, loss: 1.72838\n",
      "[00:40:15][4.71 s] train | ep:   3, it:   275, loss: 1.68694\n",
      "[00:40:19][4.43 s] train | ep:   3, it:   280, loss: 1.70316\n",
      "[00:40:24][4.53 s] train | ep:   3, it:   285, loss: 1.69891\n",
      "[00:40:28][4.60 s] train | ep:   3, it:   290, loss: 1.76900\n",
      "[00:40:33][4.22 s] train | ep:   3, it:   295, loss: 1.73671\n",
      "[00:40:37][4.49 s] train | ep:   3, it:   300, loss: 1.69552\n",
      "[00:40:39][1.36 s]   val | ep:   3, it:     5, loss: 1.71022\n",
      "  val | ep: 3, acc: 0.400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 300\n",
       "\tepoch: 3\n",
       "\tepoch_length: 100\n",
       "\tmax_epochs: 3\n",
       "\toutput: <class 'dict'>\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: 12\n",
       "\tep_time: 1584049237.6555922\n",
       "\ttime: 1584049239.0117607"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_dl, max_epochs=3, epoch_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = iter(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = batch[0].permute(0, 2, 3, 4, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(images[:,1], cols=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
