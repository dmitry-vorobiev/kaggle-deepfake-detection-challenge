{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from numba import jit, njit\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'data/dfdc-crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import ops\n",
    "\n",
    "import nvidia.dali as dali\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "# src\n",
    "sys.path.insert(0, SRC_DIR)\n",
    "from sample.reader import VideoReader\n",
    "from dataset.utils import read_labels\n",
    "\n",
    "# Pytorch_Retinaface\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'Pytorch_Retinaface'))\n",
    "from data import cfg_mnet\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from models.retinaface import RetinaFace\n",
    "from detect_utils import detect, load_model, postproc_detections, postproc_frame\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_axis(c0, c1, pad, cmax):\n",
    "    c0 = max(0, c0 - pad)\n",
    "    c1 = min(cmax, c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "@njit\n",
    "def expand_bbox(bbox, pct):\n",
    "    bbox = np.copy(bbox)\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@njit\n",
    "def crop_face(img, bbox, pad_pct=0.05):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    \n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox(bbox, pad_pct)\n",
    "        \n",
    "    x0, y0, x1, y1 = bbox.astype(np.int16)\n",
    "    \n",
    "    w, h = x1 - x0, y1 - y0\n",
    "    if w > h:\n",
    "        pad = (w - h) // 2\n",
    "        y0, y1, h = calc_axis(y0, y1, pad, img_h)\n",
    "    elif h > w:\n",
    "        pad = (h - w) // 2\n",
    "        x0, x1, w = calc_axis(x0, x1, pad, img_w)\n",
    "    \n",
    "    size = min(w, h)\n",
    "    face = img[y0:y1, x0:x1][:size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_num_faces(num_faces, frac_thresh=0.25):\n",
    "    avg = num_faces.mean()\n",
    "    fraction, integral = np.modf(avg)\n",
    "    rounded = integral if fraction < frac_thresh else integral + 1\n",
    "    return int(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPipe(dali.pipeline.Pipeline):\n",
    "    def __init__(self, file_list: str, seq_len=30, stride=10, \n",
    "                 batch_size=1, num_threads=1, device_id=0):\n",
    "        super(VideoPipe, self).__init__(\n",
    "            batch_size, num_threads, device_id, seed=3)\n",
    "        \n",
    "        self.input = dali.ops.VideoReader(\n",
    "            device='gpu', file_list=file_list, sequence_length=seq_len, \n",
    "            stride=stride, shard_id=0, num_shards=1)\n",
    "\n",
    "    def define_graph(self):\n",
    "        output, labels = self.input(name='reader')\n",
    "        return output, labels\n",
    "    \n",
    "    \n",
    "def get_file_list(df: pd.DataFrame, start: int, end: int, \n",
    "                  base_dir: str =DATA_DIR) -> List[str]:\n",
    "    path_fn = lambda row: os.path.join(base_dir, row.dir, row.name)\n",
    "    return df.iloc[start:end].apply(path_fn, axis=1).values.tolist()\n",
    "\n",
    "\n",
    "def write_file_list(files: List[str], path='./file_list.txt') -> None:    \n",
    "    with open(path, mode='w') as h:\n",
    "        for i, f in enumerate(files):\n",
    "            h.write(f'{f} {i}\\n')\n",
    "\n",
    "\n",
    "def init_detector(cfg, weights, use_cpu=False):\n",
    "    cfg['pretrain'] = False\n",
    "    net = RetinaFace(cfg=cfg, phase='test')\n",
    "    net = load_model(net, weights, use_cpu)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def mkdirs(base_dir, chunk_dirs):\n",
    "    for chunk_dir in chunk_dirs:\n",
    "        dir_path = os.path.join(base_dir, chunk_dir)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imgs(sample: Union[np.ndarray, Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "    n, h, w, c = sample.shape\n",
    "    mean = [104, 117, 123]\n",
    "    if isinstance(sample, Tensor):\n",
    "        imgs = sample.float()\n",
    "        imgs -= torch.tensor(mean, device=imgs.device)\n",
    "        imgs = imgs.permute(0, 3, 1, 2)\n",
    "    else:\n",
    "        imgs = np.float32(sample)\n",
    "        imgs -= mean\n",
    "        imgs = imgs.transpose(0, 3, 1, 2)\n",
    "        imgs = torch.from_numpy(imgs)\n",
    "    scale = torch.tensor([w, h, w, h])\n",
    "    return imgs, scale\n",
    "\n",
    "\n",
    "def detect(sample: Union[np.ndarray, Tensor], model: torch.nn.Module, \n",
    "           cfg: Dict[str,any], device: torch.device) -> List[np.ndarray]:\n",
    "    bs = cfg['batch_size']\n",
    "    num_frames, height, width, ch = sample.shape\n",
    "    imgs, scale = prepare_imgs(sample)\n",
    "    \n",
    "    priorbox = PriorBox(cfg, image_size=(height, width))\n",
    "    priors = priorbox.forward().to(device)\n",
    "    scale = scale.to(device)\n",
    "    detections = []\n",
    "    \n",
    "    for start in range(0, num_frames, bs):\n",
    "        end = start + bs\n",
    "        imgs_batch = imgs[start:end].to(device)\n",
    "        with torch.no_grad():\n",
    "            loc, conf, landms = model(imgs_batch)\n",
    "        imgs_batch, landms = None, None\n",
    "        dets = postproc_detections(loc, conf, priors, scale, cfg)\n",
    "        detections.extend(dets)\n",
    "        loc, conf = None, None\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_cv2(path: str, num_frames: int, jitter=0, seed=None) -> np.ndarray:\n",
    "    \"\"\"Reads frames that are always evenly spaced throughout the video.\n",
    "\n",
    "    Arguments:\n",
    "        path: the video file\n",
    "        num_frames: how many frames to read, -1 means the entire video\n",
    "            (warning: this will take up a lot of memory!)\n",
    "        jitter: if not 0, adds small random offsets to the frame indices;\n",
    "            this is useful so we don't always land on even or odd frames\n",
    "        seed: random seed for jittering; if you set this to a fixed value,\n",
    "            you probably want to set it only on the first video \n",
    "    \"\"\"\n",
    "    capture = cv2.VideoCapture(path)\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count <= 0: \n",
    "        return None\n",
    "    frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=np.int)\n",
    "    if jitter > 0:\n",
    "        np.random.seed(seed)\n",
    "        jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
    "        frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
    "    result = read_frames_at_indices(path, capture, frame_idxs)\n",
    "    capture.release()\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_frames_at_indices(\n",
    "        path: str, capture: cv2.VideoCapture, frame_idxs: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        frames = []\n",
    "        next_idx = 0\n",
    "        for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
    "            ret = capture.grab()\n",
    "            if not ret:\n",
    "                print('Unable to grab frame %d from %s' % (frame_idx, path))\n",
    "                break\n",
    "            if frame_idx == frame_idxs[next_idx]:\n",
    "                ret, frame = capture.retrieve()\n",
    "                if not ret or frame is None:\n",
    "                    print('Unable to retrieve frame %d from %s' % (frame_idx, path))\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "                next_idx += 1\n",
    "        if len(frames) > 0:\n",
    "            return np.stack(frames)\n",
    "        else:\n",
    "            print('No frames have been read from %s' % path)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print('Unable to read %s' % path)\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "\n",
    "def save_images_npz(path: str, images: List[np.ndarray]) -> None:\n",
    "    file_map = {str(i):cv2.imencode('.png', image)[1] \n",
    "                for i,face in enumerate(images)}\n",
    "    np.savez_compressed(path, **file_map)\n",
    "    \n",
    "    \n",
    "def save_images_hdf5(path: str, images: List[np.ndarray]) -> None:\n",
    "    with h5py.File(file_path, 'w') as file:    \n",
    "        for i, image in enumerate(images):\n",
    "            dataset = file.create_dataset(\n",
    "                str(i), data=cv2.imencode('.png', image)[1], \n",
    "                compression='gzip', compression_opts=6)\n",
    "    \n",
    "    \n",
    "def load_images_from_npz(path: str) -> List[np.ndarray]:\n",
    "    images = []\n",
    "    with np.load(path) as archive:\n",
    "        for key in archive.keys():\n",
    "            img = cv2.imdecode(archive[key], cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def frames_to_faces(\n",
    "        frames: np.ndarray, detect_fn, frames_dir: str) -> None:\n",
    "    if not os.path.isdir(frames_dir):\n",
    "        os.mkdir(frames_dir)\n",
    "    detections = detect_fn(frames)\n",
    "    if isinstance(frames, Tensor):\n",
    "        frames = frames.cpu().numpy()\n",
    "    num_faces = np.array(list(map(len, detections)), dtype=np.uint8)\n",
    "    max_faces_per_frame = round_num_faces(num_faces, frac_thresh=0.25)\n",
    "    \n",
    "    faces = []\n",
    "    for f in range(len(frames)):\n",
    "        for det in detections[f][:max_faces_per_frame]:\n",
    "            face = crop_face(frames[f], det[:4])\n",
    "            file_path = os.path.join(frames_dir, '%03d.png' % f)\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "            faces.append(face)\n",
    "            # cv2.imwrite(file_path, face)\n",
    "    detections = None\n",
    "    # dir_path, filename = os.path.split(frames_dir)\n",
    "    # file_path = os.path.join(dir_path, filename+'.h5')\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "        start=0, end=None, chunk_dirs=None, max_open_files=100, \n",
    "        file_list_path='./temp_files.txt', verbose=False,\n",
    "        num_frames=30, stride=10, use_cpu=False, bs=32, \n",
    "        base_dir=BASE_DIR, data_dir=DATA_DIR, save_dir=SAVE_DIR):\n",
    "    df = read_labels(data_dir, chunk_dirs=chunk_dirs)\n",
    "    mkdirs(save_dir, df['dir'].unique())\n",
    "    \n",
    "    device = torch.device(\"cpu\" if use_cpu else \"cuda\")\n",
    "    weights_mnet = os.path.join(base_dir, 'data/weights/mobilenet0.25_Final.pth')\n",
    "    cfg = {**cfg_mnet, 'batch_size': bs}\n",
    "    detector = init_detector(cfg, weights_mnet, use_cpu).to(device)\n",
    "    detect_fn = partial(detect, model=detector, cfg=cfg, device=device)\n",
    "    \n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "    \n",
    "    for start_pos in range(start, end, max_open_files):\n",
    "        end_pos = min(start_pos + max_open_files, end)\n",
    "        files = get_file_list(df, start_pos, end_pos)\n",
    "        write_file_list(files, path=file_list_path)\n",
    "        pipe = VideoPipe(file_list_path, seq_len=num_frames, stride=stride)\n",
    "        pipe.build()\n",
    "        num_samples = len(files)\n",
    "        num_samples_read = pipe.epoch_size('reader')\n",
    "        num_bad_samples = num_samples - num_samples_read\n",
    "        run_fallback_reader = num_bad_samples > 0\n",
    "        if run_fallback_reader:\n",
    "            print('Unable to parse %d videos with DALI' % num_bad_samples)\n",
    "            proc_file_idxs = np.zeros(num_samples, dtype=np.bool)\n",
    "            \n",
    "        if num_samples_read > 0:\n",
    "            data_iter = DALIGenericIterator(\n",
    "                [pipe], ['frames', 'label'], num_samples_read, dynamic_shape=True)\n",
    "            if verbose: \n",
    "                t0 = time.time()\n",
    "            for idx, video_batch in tqdm(enumerate(data_iter), total=num_samples_read):\n",
    "                frames = video_batch[0]['frames'].squeeze(0)\n",
    "                read_idx =  video_batch[0]['label'].item()\n",
    "                abs_idx = start_pos + read_idx\n",
    "                meta = df.iloc[abs_idx]\n",
    "                sample_dir = os.path.join(save_dir, meta.dir, meta.name[:-4])\n",
    "                frames_to_faces(frames, detect_fn, sample_dir)\n",
    "                if run_fallback_reader:\n",
    "                    proc_file_idxs[read_idx] = True\n",
    "                if verbose:\n",
    "                    t1 = time.time()\n",
    "                    print('[%6d][%.02f s] %s' % (abs_idx, t1 - t0, sample_dir))\n",
    "                    t0 = t1\n",
    "                    \n",
    "        pipe, data_iter = None, None\n",
    "        gc.collect()\n",
    "        \n",
    "        if run_fallback_reader:\n",
    "            print('Running fallback decoding through OpenCV...')\n",
    "            unproc_file_idxs = (~proc_file_idxs).nonzero()[0]\n",
    "            for idx in tqdm(unproc_file_idxs):\n",
    "                if verbose: \n",
    "                    t0 = time.time()\n",
    "                frames = read_frames_cv2(files[idx], num_frames)\n",
    "                abs_idx = start_pos + idx\n",
    "                meta = df.iloc[abs_idx]\n",
    "                sample_dir = os.path.join(save_dir, meta.dir, meta.name[:-4])\n",
    "                frames_to_faces(frames, detect_fn, sample_dir)\n",
    "                if verbose:\n",
    "                    t1 = time.time()\n",
    "                    print('[%6d][%.02f s] %s' % (abs_idx, t1 - t0, sample_dir))\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/dmitry/projects/dfdc/data/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5a48912cd94104afa358377832ce41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0][1.03 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/qyyebirxwe\n",
      "[     1][0.58 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/ntjlknlcvn\n",
      "[     2][0.63 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/qivpypiwlp\n",
      "[     3][0.62 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/lpkgabskbw\n",
      "[     4][0.75 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/vctemjbusz\n",
      "[     5][0.74 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/kligjmklcw\n",
      "[     6][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/jtteetuqtt\n",
      "[     7][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/bbuebaojot\n",
      "[     8][0.77 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/jsnkotxoua\n",
      "[     9][0.77 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/uazbhwyysx\n",
      "\n",
      "DONE\n",
      "CPU times: user 7.82 s, sys: 1.82 s, total: 9.64 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "prepare_data(start=0, end=10, max_open_files=300, bs=30, verbose=True, \n",
    "             chunk_dirs=['dfdc_train_part_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/jsnkotxoua.h5.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e0d2c2bc9f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/jsnkotxoua.h5.npz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_from_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7d7a20396e7d>\u001b[0m in \u001b[0;36mload_images_from_npz\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images_from_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/jsnkotxoua.h5.npz'"
     ]
    }
   ],
   "source": [
    "path = '/home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_2/jsnkotxoua.h5.npz'\n",
    "faces = load_images_from_npz(path)        \n",
    "show_images(faces, cols=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
