{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from numba import jit, njit\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'data/dfdc-crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import ops\n",
    "\n",
    "import nvidia.dali as dali\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "# src\n",
    "sys.path.insert(0, SRC_DIR)\n",
    "from sample.reader import VideoReader\n",
    "from dataset.utils import read_labels\n",
    "\n",
    "# Pytorch_Retinaface\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'Pytorch_Retinaface'))\n",
    "from data import cfg_mnet\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from models.retinaface import RetinaFace\n",
    "from detect_utils import detect, load_model, postproc_detections, postproc_frame\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_axis(c0, c1, pad, cmax):\n",
    "    c0 = max(0, c0 - pad)\n",
    "    c1 = min(cmax, c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "@njit\n",
    "def expand_bbox(bbox, pct):\n",
    "    bbox = np.copy(bbox)\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@njit\n",
    "def crop_face(img, bbox, pad_pct=0.05, square=True):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    \n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox(bbox, pad_pct)\n",
    "        \n",
    "    x0, y0, x1, y1 = bbox.astype(np.int16)\n",
    "    \n",
    "    if square:\n",
    "        w, h = x1 - x0, y1 - y0\n",
    "        if w > h:\n",
    "            pad = (w - h) // 2\n",
    "            y0, y1, h = calc_axis(y0, y1, pad, img_h)\n",
    "        elif h > w:\n",
    "            pad = (h - w) // 2\n",
    "            x0, x1, w = calc_axis(x0, x1, pad, img_w)\n",
    "    \n",
    "    size = min(w, h)\n",
    "    face = img[y0:y1, x0:x1][:size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_num_faces(num_faces, frac_thresh=0.25):\n",
    "    avg = num_faces.mean()\n",
    "    fraction, integral = np.modf(avg)\n",
    "    rounded = integral if fraction < frac_thresh else integral + 1\n",
    "    return int(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPipe(dali.pipeline.Pipeline):\n",
    "    def __init__(self, file_list: str, seq_len=30, stride=10, \n",
    "                 batch_size=1, num_threads=1, device_id=0):\n",
    "        super(VideoPipe, self).__init__(\n",
    "            batch_size, num_threads, device_id, seed=3)\n",
    "        \n",
    "        self.input = dali.ops.VideoReader(\n",
    "            device='gpu', file_list=file_list, sequence_length=seq_len, \n",
    "            stride=stride, shard_id=0, num_shards=1)\n",
    "\n",
    "    def define_graph(self):\n",
    "        output, labels = self.input(name='reader')\n",
    "        return output, labels\n",
    "    \n",
    "    \n",
    "def get_file_list(df: pd.DataFrame, start: int, end: int, \n",
    "                  base_dir: str =DATA_DIR) -> List[str]:\n",
    "    path_fn = lambda row: os.path.join(base_dir, row.dir, row.name)\n",
    "    return df.iloc[start:end].apply(path_fn, axis=1).values.tolist()\n",
    "\n",
    "\n",
    "def write_file_list(files: List[str], path='./file_list.txt') -> None:    \n",
    "    with open(path, mode='w') as h:\n",
    "        for i, f in enumerate(files):\n",
    "            h.write(f'{f} {i}\\n')\n",
    "\n",
    "\n",
    "def init_detector(cfg, weights, use_cpu=False):\n",
    "    cfg['pretrain'] = False\n",
    "    net = RetinaFace(cfg=cfg, phase='test')\n",
    "    net = load_model(net, weights, use_cpu)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def mkdirs(base_dir, chunk_dirs):\n",
    "    for chunk_dir in chunk_dirs:\n",
    "        dir_path = os.path.join(base_dir, chunk_dir)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imgs(sample: Union[np.ndarray, Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "    n, h, w, c = sample.shape\n",
    "    mean = [104, 117, 123]\n",
    "    if isinstance(sample, Tensor):\n",
    "        imgs = sample.float()\n",
    "        imgs -= torch.tensor(mean, device=imgs.device)\n",
    "        imgs = imgs.permute(0, 3, 1, 2)\n",
    "    else:\n",
    "        imgs = np.float32(sample)\n",
    "        imgs -= mean\n",
    "        imgs = imgs.transpose(0, 3, 1, 2)\n",
    "        imgs = torch.from_numpy(imgs)\n",
    "    scale = torch.tensor([w, h, w, h])\n",
    "    return imgs, scale\n",
    "\n",
    "\n",
    "def detect(sample: Union[np.ndarray, Tensor], model: torch.nn.Module, \n",
    "           cfg: Dict[str,any], device: torch.device) -> List[np.ndarray]:\n",
    "    bs = cfg['batch_size']\n",
    "    num_frames, height, width, ch = sample.shape\n",
    "    imgs, scale = prepare_imgs(sample)\n",
    "    \n",
    "    priorbox = PriorBox(cfg, image_size=(height, width))\n",
    "    priors = priorbox.forward().to(device)\n",
    "    scale = scale.to(device)\n",
    "    detections = []\n",
    "    \n",
    "    for start in range(0, num_frames, bs):\n",
    "        end = start + bs\n",
    "        imgs_batch = imgs[start:end].to(device)\n",
    "        with torch.no_grad():\n",
    "            loc, conf, landms = model(imgs_batch)\n",
    "        imgs_batch, landms = None, None\n",
    "        dets = postproc_detections(loc, conf, priors, scale, cfg)\n",
    "        detections.extend(dets)\n",
    "        loc, conf = None, None\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_cv2(path: str, num_frames: int, jitter=0, seed=None) -> np.ndarray:\n",
    "    \"\"\"Reads frames that are always evenly spaced throughout the video.\n",
    "\n",
    "    Arguments:\n",
    "        path: the video file\n",
    "        num_frames: how many frames to read, -1 means the entire video\n",
    "            (warning: this will take up a lot of memory!)\n",
    "        jitter: if not 0, adds small random offsets to the frame indices;\n",
    "            this is useful so we don't always land on even or odd frames\n",
    "        seed: random seed for jittering; if you set this to a fixed value,\n",
    "            you probably want to set it only on the first video \n",
    "    \"\"\"\n",
    "    capture = cv2.VideoCapture(path)\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count <= 0: \n",
    "        return None\n",
    "    frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=np.int)\n",
    "    if jitter > 0:\n",
    "        np.random.seed(seed)\n",
    "        jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
    "        frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
    "    result = read_frames_at_indices(path, capture, frame_idxs)\n",
    "    capture.release()\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_frames_at_indices(\n",
    "        path: str, capture: cv2.VideoCapture, frame_idxs: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        frames = []\n",
    "        next_idx = 0\n",
    "        for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
    "            ret = capture.grab()\n",
    "            if not ret:\n",
    "                print('Unable to grab frame %d from %s' % (frame_idx, path))\n",
    "                break\n",
    "            if frame_idx == frame_idxs[next_idx]:\n",
    "                ret, frame = capture.retrieve()\n",
    "                if not ret or frame is None:\n",
    "                    print('Unable to retrieve frame %d from %s' % (frame_idx, path))\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "                next_idx += 1\n",
    "        if len(frames) > 0:\n",
    "            return np.stack(frames)\n",
    "        else:\n",
    "            print('No frames have been read from %s' % path)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print('Unable to read %s' % path)\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_faces(\n",
    "        frames: np.ndarray, detect_fn, frames_dir: str) -> None:\n",
    "    if not os.path.isdir(frames_dir):\n",
    "        os.mkdir(frames_dir)\n",
    "    detections = detect_fn(frames)\n",
    "    if isinstance(frames, Tensor):\n",
    "        frames = frames.cpu().numpy()\n",
    "    num_faces = np.array(list(map(len, detections)), dtype=np.uint8)\n",
    "    max_faces_per_frame = round_num_faces(num_faces, frac_thresh=0.25)\n",
    "    for f in range(len(frames)):\n",
    "        for det in detections[f][:max_faces_per_frame]:\n",
    "            face = crop_face(frames[f], det[:4])\n",
    "            file_path = os.path.join(frames_dir, '%03d.png' % f)\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "            # cv2.imwrite(file_path, face)\n",
    "    detections = None\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "        start=0, end=None, chunk_dirs=None, max_open_files=100, \n",
    "        file_list_path='./temp_files.txt', verbose=False,\n",
    "        num_frames=30, stride=10, use_cpu=False, bs=32, \n",
    "        base_dir=BASE_DIR, data_dir=DATA_DIR, save_dir=SAVE_DIR):\n",
    "    df = read_labels(data_dir, chunk_dirs=chunk_dirs)\n",
    "    mkdirs(save_dir, df['dir'].unique())\n",
    "    \n",
    "    device = torch.device(\"cpu\" if use_cpu else \"cuda\")\n",
    "    weights_mnet = os.path.join(base_dir, 'data/weights/mobilenet0.25_Final.pth')\n",
    "    cfg = {**cfg_mnet, 'batch_size': bs}\n",
    "    detector = init_detector(cfg, weights_mnet, use_cpu).to(device)\n",
    "    detect_fn = partial(detect, model=detector, cfg=cfg, device=device)\n",
    "    \n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "    \n",
    "    for start_pos in range(start, end, max_open_files):\n",
    "        end_pos = min(start_pos + max_open_files, end)\n",
    "        files = get_file_list(df, start_pos, end_pos)\n",
    "        write_file_list(files, path=file_list_path)\n",
    "        pipe = VideoPipe(file_list_path, seq_len=num_frames, stride=stride)\n",
    "        pipe.build()\n",
    "        num_samples = len(files)\n",
    "        num_samples_read = pipe.epoch_size('reader')\n",
    "        num_bad_samples = num_samples - num_samples_read\n",
    "        run_fallback_reader = num_bad_samples > 0\n",
    "        if run_fallback_reader:\n",
    "            print('Unable to parse %d videos with DALI' % num_bad_samples)\n",
    "            proc_file_idxs = np.zeros(num_samples, dtype=np.bool)\n",
    "            \n",
    "        if num_samples_read > 0:\n",
    "            data_iter = DALIGenericIterator(\n",
    "                [pipe], ['frames', 'label'], num_samples_read, dynamic_shape=True)\n",
    "            if verbose: \n",
    "                t0 = time.time()\n",
    "            for idx, video_batch in tqdm(enumerate(data_iter), total=num_samples_read):\n",
    "                frames = video_batch[0]['frames'].squeeze(0)\n",
    "                read_idx =  video_batch[0]['label'].item()\n",
    "                abs_idx = start_pos + read_idx\n",
    "                meta = df.iloc[abs_idx]\n",
    "                sample_dir = os.path.join(save_dir, meta.dir, meta.name[:-4])\n",
    "                frames_to_faces(frames, detect_fn, sample_dir)\n",
    "                if run_fallback_reader:\n",
    "                    proc_file_idxs[read_idx] = True\n",
    "                if verbose:\n",
    "                    t1 = time.time()\n",
    "                    print('[%6d][%.02f s] %s' % (abs_idx, t1 - t0, sample_dir))\n",
    "                    t0 = t1\n",
    "                    \n",
    "        pipe, data_iter = None, None\n",
    "        gc.collect()\n",
    "        \n",
    "        if run_fallback_reader:\n",
    "            print('Running fallback decoding through OpenCV...')\n",
    "            unproc_file_idxs = (~proc_file_idxs).nonzero()[0]\n",
    "            for idx in tqdm(unproc_file_idxs):\n",
    "                if verbose: \n",
    "                    t0 = time.time()\n",
    "                frames = read_frames_cv2(files[idx], num_frames)\n",
    "                abs_idx = start_pos + idx\n",
    "                meta = df.iloc[abs_idx]\n",
    "                sample_dir = os.path.join(save_dir, meta.dir, meta.name[:-4])\n",
    "                frames_to_faces(frames, detect_fn, sample_dir)\n",
    "                if verbose:\n",
    "                    t1 = time.time()\n",
    "                    print('[%6d][%.02f s] %s' % (abs_idx, t1 - t0, sample_dir))\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/dmitry/projects/dfdc/data/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n",
      "Unable to parse 280 videos with DALI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f22e9b0e22043fe91c93c51d041ccc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2650][0.90 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/kvyvazqlev\n",
      "[  2651][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/vjnbtebjhv\n",
      "[  2652][0.54 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/bpiuysyyuj\n",
      "[  2653][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/zwqdszjdop\n",
      "[  2654][0.74 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/cjmfuasyog\n",
      "[  2655][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/szklntqqjo\n",
      "[  2656][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ixsabuaykm\n",
      "[  2657][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qvsskinsyi\n",
      "[  2658][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/eifdjwcjnq\n",
      "[  2659][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/rnpzpgncvx\n",
      "[  2660][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ljcjouvznz\n",
      "[  2661][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/tmfschpvyo\n",
      "[  2662][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/rfykxekxer\n",
      "[  2663][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/uyszohikoe\n",
      "[  2664][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/hoaweiathp\n",
      "[  2665][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/uxvarukjxl\n",
      "[  2666][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qbqimiiqil\n",
      "[  2667][0.68 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ncjqganhal\n",
      "[  2668][0.75 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/jeclduagbh\n",
      "[  2669][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/kyajzhnjjv\n",
      "[  2670][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/oygvvmtzjv\n",
      "[  2671][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/hlhncbtsgq\n",
      "[  2672][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/rdhltwxkwk\n",
      "[  2673][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/bdynuzrqbt\n",
      "[  2674][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/vwgjdpxisr\n",
      "[  2675][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/kyoihwhooi\n",
      "[  2676][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qftwzgqden\n",
      "[  2677][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xgwelqejmi\n",
      "[  2678][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qhcrglyfcc\n",
      "[  2679][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/notesdzqcd\n",
      "[  2680][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/tsycwnssva\n",
      "[  2681][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/eprdgmcksq\n",
      "[  2682][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xiuycgppds\n",
      "[  2683][0.74 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/cpjiflbqbo\n",
      "[  2684][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ulfzoinpcs\n",
      "[  2685][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/okyufzgvay\n",
      "[  2966][0.66 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/azmheosoed\n",
      "[  2967][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/iarowhjwvl\n",
      "[  2968][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/osgxmzpumw\n",
      "[  2969][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qaubhdsftl\n",
      "[  2970][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/fgkpembbpv\n",
      "[  2971][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/cimlotvwxy\n",
      "[  2972][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/avttogwbyz\n",
      "[  2973][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xbfpoccnhz\n",
      "[  2974][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/vsiqfnwpqr\n",
      "[  2975][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xrrrawsepu\n",
      "[  2976][0.77 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/tfhrnoapsg\n",
      "[  2977][0.68 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/zoqusptprh\n",
      "[  2978][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ovvoywlhig\n",
      "[  2979][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/lzdtgzczma\n",
      "[  2980][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/swbiogmgun\n",
      "[  2981][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ntqzfgmhnj\n",
      "[  2982][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/hinogxnrhx\n",
      "[  2983][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xbzswkjvyx\n",
      "[  2984][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/nigytcchgw\n",
      "[  2985][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/njradgdzmj\n",
      "[  2986][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/apansjedym\n",
      "[  2987][0.72 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/mtighbgbcy\n",
      "[  2988][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/kemappvqpu\n",
      "[  2989][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/wvhsizqfqc\n",
      "[  2990][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/agblhurdrh\n",
      "[  2991][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/kfihjlyzlv\n",
      "[  2992][0.73 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/gitefceugq\n",
      "[  2993][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/nqctdktbtb\n",
      "[  2994][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/dijkbxqbtk\n",
      "[  2995][0.70 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/bmtbxkjkjq\n",
      "[  2996][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/mbpbbekqks\n",
      "[  2997][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ahanoygyei\n",
      "[  2998][0.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xgpqvqoxgd\n",
      "[  2999][0.71 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/udqmtqsfgp\n",
      "\n",
      "Running fallback decoding through OpenCV...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc1e9faa8634a0f8c600756f0e1ed3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=280.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to grab frame 142 from /home/dmitry/projects/dfdc/data/dfdc-videos/dfdc_train_part_49/onhhgzsrvo.mp4\n",
      "[  2686][1.46 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/onhhgzsrvo\n",
      "[  2687][1.58 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ceelmfvdxi\n",
      "Unable to grab frame 111 from /home/dmitry/projects/dfdc/data/dfdc-videos/dfdc_train_part_49/mrmaezpwoc.mp4\n",
      "[  2688][1.38 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/mrmaezpwoc\n",
      "[  2689][1.62 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/yxlkovjssa\n",
      "[  2690][1.62 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ofrxxtvzsv\n",
      "Unable to grab frame 111 from /home/dmitry/projects/dfdc/data/dfdc-videos/dfdc_train_part_49/scuffekebm.mp4\n",
      "[  2691][1.35 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/scuffekebm\n",
      "Unable to grab frame 111 from /home/dmitry/projects/dfdc/data/dfdc-videos/dfdc_train_part_49/fywvtiicud.mp4\n",
      "[  2692][1.34 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/fywvtiicud\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f910eed30302>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(start, end, chunk_dirs, max_open_files, file_list_path, verbose, num_frames, stride, use_cpu, bs, base_dir, data_dir, save_dir)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabs_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0msample_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mframes_to_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f910eed30302>\u001b[0m in \u001b[0;36mframes_to_faces\u001b[0;34m(frames, detect_fn, frames_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-dc51550c8558>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(sample, model, cfg, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mimgs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostproc_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/dfdc/Pytorch_Retinaface/detect_utils.py\u001b[0m in \u001b[0;36mpostproc_detections\u001b[0;34m(locations, confidence, priors, scale, cfg, resize)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'variance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "prepare_data(start=2650, end=3000, max_open_files=350, bs=30, verbose=True, \n",
    "             chunk_dirs=['dfdc_train_part_49'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
