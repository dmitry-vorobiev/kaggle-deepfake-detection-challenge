{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'data/dfdc-crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# src\n",
    "sys.path.insert(0, SRC_DIR)\n",
    "from sample.reader import VideoReader\n",
    "from dataset.utils import read_labels\n",
    "\n",
    "# Pytorch_Retinaface\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'Pytorch_Retinaface'))\n",
    "from data import cfg_mnet\n",
    "from models.retinaface import RetinaFace\n",
    "from detect_utils import detect, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(idx, labels_df, reader, n_frames=10, base_dir=DATA_DIR):\n",
    "    row = labels_df.iloc[idx]\n",
    "    file_path = os.path.join(base_dir, row.dir, row.name)\n",
    "    sample, frames = reader.read_frames(file_path, n_frames)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def get_text_label(idx, labels_df):\n",
    "    label = labels_df.iloc[idx]['label']\n",
    "    label = 'FAKE' if label else 'REAL'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_axis(c0, c1, pad, cmax):\n",
    "    c0 = max(0, c0 - pad)\n",
    "    c1 = min(cmax, c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "@njit\n",
    "def expand_bbox(bbox, pct):\n",
    "    bbox = np.copy(bbox)\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@njit\n",
    "def crop_face(img, bbox, pad_pct=0.05, square=True):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    \n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox(bbox, pad_pct)\n",
    "        \n",
    "    x0, y0, x1, y1 = bbox.astype(np.int16)\n",
    "    \n",
    "    if square:\n",
    "        w, h = x1 - x0, y1 - y0\n",
    "        if w > h:\n",
    "            pad = (w - h) // 2\n",
    "            y0, y1, h = calc_axis(y0, y1, pad, img_h)\n",
    "        elif h > w:\n",
    "            pad = (h - w) // 2\n",
    "            x0, x1, w = calc_axis(x0, x1, pad, img_w)\n",
    "    \n",
    "    size = min(w, h)\n",
    "    face = img[y0:y1, x0:x1][:size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_num_faces(num_faces, frac_thresh=0.25):\n",
    "    avg = num_faces.mean()\n",
    "    fraction, integral = np.modf(avg)\n",
    "    rounded = integral if fraction < frac_thresh else integral + 1\n",
    "    return int(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mobilenet0.25',\n",
       " 'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
       " 'steps': [8, 16, 32],\n",
       " 'variance': [0.1, 0.2],\n",
       " 'clip': False,\n",
       " 'loc_weight': 2.0,\n",
       " 'gpu_train': True,\n",
       " 'batch_size': 32,\n",
       " 'ngpu': 1,\n",
       " 'epoch': 250,\n",
       " 'decay1': 190,\n",
       " 'decay2': 220,\n",
       " 'image_size': 640,\n",
       " 'pretrain': True,\n",
       " 'return_layers': {'stage1': 1, 'stage2': 2, 'stage3': 3},\n",
       " 'in_channel': 32,\n",
       " 'out_channel': 64}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_mnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_detector(cfg, weights, use_cpu=False):\n",
    "    cfg['pretrain'] = False\n",
    "    net = RetinaFace(cfg=cfg, phase='test')\n",
    "    net = load_model(net, weights, use_cpu)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def mkdirs(base_dir, chunk_dirs):\n",
    "    for chunk_dir in chunk_dirs:\n",
    "        dir_path = os.path.join(base_dir, chunk_dir)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "        start=0, end=None, \n",
    "        num_frames_fake=30, num_frames_real=120,\n",
    "        use_cpu=False, bs=32, verbose=False,\n",
    "        base_dir=BASE_DIR, data_dir=DATA_DIR, save_dir=SAVE_DIR):\n",
    "    df = read_labels(data_dir)\n",
    "    mkdirs(save_dir, df['dir'].unique())\n",
    "    \n",
    "    reader = VideoReader()\n",
    "    device = torch.device(\"cpu\" if use_cpu else \"cuda\")\n",
    "    weights_mnet = os.path.join(base_dir, 'data/weights/mobilenet0.25_Final.pth')\n",
    "    cfg = {**cfg_mnet, 'batch_size': bs}\n",
    "    detector = init_detector(cfg, weights_mnet, use_cpu).to(device)\n",
    "    \n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "        \n",
    "    for idx in tqdm(range(start, end), total=(end-start)):\n",
    "        meta = df.iloc[idx]\n",
    "        fake = bool(meta['label'])\n",
    "        \n",
    "        sample_dir = os.path.join(save_dir, meta.dir, meta.name[:-4])\n",
    "        if not os.path.isdir(sample_dir):\n",
    "            os.mkdir(sample_dir)\n",
    "        if verbose:\n",
    "            t0 = time.time()\n",
    "            \n",
    "        num_frames = num_frames_fake if fake else num_frames_real\n",
    "        sample = get_sample(idx, df, reader, n_frames=num_frames)\n",
    "        detections = detect(sample, detector, cfg_mnet, device)\n",
    "        num_faces = np.array(list(map(len, detections)), dtype=np.uint8)\n",
    "        max_faces_per_frame = round_num_faces(num_faces, frac_thresh=0.25)\n",
    "    \n",
    "        for f in range(num_frames):\n",
    "            for det in detections[f][:max_faces_per_frame]:\n",
    "                face = crop_face(sample[f], det[:4])\n",
    "                file_path = os.path.join(sample_dir, '%03d.png' % f)\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "                # cv2.imwrite(file_path, face)\n",
    "        detections = None\n",
    "        # gc.collect()\n",
    "        \n",
    "        if verbose:\n",
    "            t1 = time.time()\n",
    "            print('[%.02f s] %s' % (t1 - t0, sample_dir))\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/dmitry/projects/dfdc/data/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5fe1dc43e14a83b7444b81d93cd3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.85 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/vkketnrfud\n",
      "[1.83 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/tnsaqegyqt\n",
      "[5.69 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/jcwkemycdm\n",
      "[1.83 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/lnpsnoufkq\n",
      "[1.82 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/rdfdbmyrqm\n",
      "[1.89 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/otyrbsrkhn\n",
      "[1.84 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/zmlpmfbryq\n",
      "[5.58 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/sjhdwvfdbi\n",
      "[1.74 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/npbreznxbl\n",
      "[1.82 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/khoogmqdci\n",
      "[1.87 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_22/zewjjvygcr\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-823c5c2be4c6>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(start, end, num_frames_fake, num_frames_real, use_cpu, bs, verbose, base_dir, data_dir, save_dir)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_frames_fake\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfake\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnum_frames_real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg_mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mnum_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmax_faces_per_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround_num_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/dfdc/Pytorch_Retinaface/detect_utils.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(sample, model, cfg, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimgs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mdets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostproc_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/dfdc/Pytorch_Retinaface/detect_utils.py\u001b[0m in \u001b[0;36mpostproc_detections\u001b[0;34m(locations, confidence, priors, scale, cfg, resize)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'variance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "prepare_data(start=0, end=2500, bs=30, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs224n] *",
   "language": "python",
   "name": "conda-env-cs224n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
