{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from numba import jit, njit\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'data/dfdc-crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import ops\n",
    "\n",
    "import nvidia.dali as dali\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "# src\n",
    "sys.path.insert(0, SRC_DIR)\n",
    "from sample.reader import VideoReader\n",
    "from dataset.utils import read_labels\n",
    "\n",
    "# Pytorch_Retinaface\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'Pytorch_Retinaface'))\n",
    "from data import cfg_mnet\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from models.retinaface import RetinaFace\n",
    "from detect_utils import detect, load_model, postproc_detections\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_axis(c0, c1, pad, cmax):\n",
    "    c0 = max(0, c0 - pad)\n",
    "    c1 = min(cmax, c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "@njit\n",
    "def expand_bbox(bbox, pct):\n",
    "    bbox = np.copy(bbox)\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@njit\n",
    "def crop_face(img, bbox, pad_pct=0.05, square=True):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    \n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox(bbox, pad_pct)\n",
    "        \n",
    "    x0, y0, x1, y1 = bbox.astype(np.int16)\n",
    "    \n",
    "    if square:\n",
    "        w, h = x1 - x0, y1 - y0\n",
    "        if w > h:\n",
    "            pad = (w - h) // 2\n",
    "            y0, y1, h = calc_axis(y0, y1, pad, img_h)\n",
    "        elif h > w:\n",
    "            pad = (h - w) // 2\n",
    "            x0, x1, w = calc_axis(x0, x1, pad, img_w)\n",
    "    \n",
    "    size = min(w, h)\n",
    "    face = img[y0:y1, x0:x1][:size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_num_faces(num_faces, frac_thresh=0.25):\n",
    "    avg = num_faces.mean()\n",
    "    fraction, integral = np.modf(avg)\n",
    "    rounded = integral if fraction < frac_thresh else integral + 1\n",
    "    return int(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPipe(dali.pipeline.Pipeline):\n",
    "    def __init__(self, filenames: List[str], seq_len=30, stride=10, \n",
    "                 batch_size=1, num_threads=1, device_id=0):\n",
    "        super(VideoPipe, self).__init__(\n",
    "            batch_size, num_threads, device_id, seed=3)\n",
    "        self.input = dali.ops.VideoReader(\n",
    "            device='gpu', filenames=filenames, \n",
    "            sequence_length=seq_len, stride=stride,\n",
    "            shard_id=0, num_shards=1)\n",
    "\n",
    "    def define_graph(self):\n",
    "        output = self.input(name='reader')\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def get_file_list(df: pd.DataFrame, start: int, end: int, \n",
    "                  base_dir:str=DATA_DIR) -> List[str]:\n",
    "    path_fn = lambda row: os.path.join(base_dir, row.dir, row.name)\n",
    "    return df.iloc[start:end].apply(path_fn, axis=1).values.tolist()\n",
    "\n",
    "\n",
    "def init_detector(cfg, weights, use_cpu=False):\n",
    "    cfg['pretrain'] = False\n",
    "    net = RetinaFace(cfg=cfg, phase='test')\n",
    "    net = load_model(net, weights, use_cpu)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def mkdirs(base_dir, chunk_dirs):\n",
    "    for chunk_dir in chunk_dirs:\n",
    "        dir_path = os.path.join(base_dir, chunk_dir)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imgs(sample):\n",
    "    n, h, w, c = sample.shape\n",
    "    imgs = sample.float()\n",
    "    imgs -= torch.tensor([104, 117, 123], device=imgs.device)\n",
    "    imgs = imgs.permute(0, 3, 1, 2)\n",
    "    scale = torch.tensor([w, h, w, h])\n",
    "    return imgs, scale\n",
    "\n",
    "\n",
    "def detect(sample, model, cfg, device):\n",
    "    bs = cfg['batch_size']\n",
    "    num_frames, height, width, ch = sample.shape\n",
    "    imgs, scale = prepare_imgs(sample)\n",
    "\n",
    "    priorbox = PriorBox(cfg, image_size=(height, width))\n",
    "    priors = priorbox.forward().to(device)\n",
    "    scale = scale.to(device)\n",
    "\n",
    "    detections = []\n",
    "    for start in range(0, num_frames, bs):\n",
    "        end = start + bs\n",
    "        imgs_batch = imgs[start:end] #.to(device)\n",
    "        with torch.no_grad():\n",
    "            loc, conf, landms = model(imgs_batch)\n",
    "        imgs_batch, landms = None, None\n",
    "        dets = postproc_detections(loc, conf, priors, scale, cfg)\n",
    "        detections.append(dets)\n",
    "        loc, conf = None, None\n",
    "    return np.vstack(detections) if len(detections) > 1 else detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "        start=0, end=None, \n",
    "        num_frames_fake=30, num_frames_real=120,\n",
    "        use_cpu=False, bs=32, verbose=False,\n",
    "        base_dir=BASE_DIR, data_dir=DATA_DIR, save_dir=SAVE_DIR,\n",
    "        chunk_dirs=None, max_open_files=100):\n",
    "    df = read_labels(data_dir, chunk_dirs=chunk_dirs)\n",
    "    mkdirs(save_dir, df['dir'].unique())\n",
    "    \n",
    "    device = torch.device(\"cpu\" if use_cpu else \"cuda\")\n",
    "    weights_mnet = os.path.join(base_dir, 'data/weights/mobilenet0.25_Final.pth')\n",
    "    cfg = {**cfg_mnet, 'batch_size': bs}\n",
    "    detector = init_detector(cfg, weights_mnet, use_cpu).to(device)\n",
    "    \n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "        \n",
    "    num_frames = num_frames_fake\n",
    "    \n",
    "    for start_pos in range(start, end, max_open_files):\n",
    "        end_pos = min(start_pos + max_open_files, end)\n",
    "        files = get_file_list(df, start_pos, end_pos)\n",
    "        pipe = VideoPipe(files, seq_len=num_frames, stride=300//num_frames)\n",
    "        pipe.build()\n",
    "        \n",
    "        data_iter = DALIGenericIterator(\n",
    "            [pipe], ['images'], len(files), dynamic_shape=True)\n",
    "        for idx, batch in tqdm(enumerate(data_iter), total=len(files)):\n",
    "            meta = df.iloc[start_pos + idx] # <- check this !!!\n",
    "            # fake = bool(meta['label'])\n",
    "\n",
    "            sample_dir = os.path.join(save_dir, meta.dir, meta.name[:-4])\n",
    "            if not os.path.isdir(sample_dir):\n",
    "                os.mkdir(sample_dir)\n",
    "            if verbose:\n",
    "                t0 = time.time()\n",
    "\n",
    "            images = batch[0]['images'].squeeze(0)\n",
    "            detections = detect(images, detector, cfg_mnet, device)\n",
    "            num_faces = np.array(list(map(len, detections)), dtype=np.uint8)\n",
    "            max_faces_per_frame = round_num_faces(num_faces, frac_thresh=0.25)\n",
    "            images = images.cpu().numpy()\n",
    "\n",
    "            for f in range(num_frames):\n",
    "                for det in detections[f][:max_faces_per_frame]:\n",
    "                    face = crop_face(images[f], det[:4])\n",
    "                    file_path = os.path.join(sample_dir, '%03d.png' % f)\n",
    "                    face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "                    # cv2.imwrite(file_path, face)\n",
    "            detections = None\n",
    "\n",
    "            if verbose:\n",
    "                t1 = time.time()\n",
    "                print('[%6d][%.02f s] %s' % (start_pos + idx, t1 - t0, sample_dir))\n",
    "                \n",
    "        files, pipe, data_iter = None, None, None\n",
    "        gc.collect()\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3135\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/dfdc-videos/dfdc_train_part_49 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = read_labels(DATA_DIR, chunk_dirs=['dfdc_train_part_49'])\n",
    "# files = get_file_list(df, 2680, 2730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/dmitry/projects/dfdc/data/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9127caf28a6e4f82a01061c086b9a03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2660][1.01 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ljcjouvznz\n",
      "[  2661][0.46 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/tmfschpvyo\n",
      "[  2662][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/rfykxekxer\n",
      "[  2663][0.46 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/uyszohikoe\n",
      "[  2664][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/hoaweiathp\n",
      "[  2665][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/uxvarukjxl\n",
      "[  2666][0.46 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qbqimiiqil\n",
      "[  2667][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ncjqganhal\n",
      "[  2668][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/jeclduagbh\n",
      "[  2669][0.46 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/kyajzhnjjv\n",
      "[  2670][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/oygvvmtzjv\n",
      "[  2671][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/hlhncbtsgq\n",
      "[  2672][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/rdhltwxkwk\n",
      "[  2673][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/bdynuzrqbt\n",
      "[  2674][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/vwgjdpxisr\n",
      "[  2675][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/kyoihwhooi\n",
      "[  2676][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qftwzgqden\n",
      "[  2677][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xgwelqejmi\n",
      "[  2678][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/qhcrglyfcc\n",
      "[  2679][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/notesdzqcd\n",
      "[  2680][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/tsycwnssva\n",
      "[  2681][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/eprdgmcksq\n",
      "[  2682][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xiuycgppds\n",
      "[  2683][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/cpjiflbqbo\n",
      "[  2684][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ulfzoinpcs\n",
      "[  2685][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/okyufzgvay\n",
      "[  2686][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/onhhgzsrvo\n",
      "[  2687][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ceelmfvdxi\n",
      "[  2688][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/mrmaezpwoc\n",
      "[  2689][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/yxlkovjssa\n",
      "[  2690][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ofrxxtvzsv\n",
      "[  2691][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/scuffekebm\n",
      "[  2692][0.46 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/fywvtiicud\n",
      "[  2693][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/rsnrcisldc\n",
      "[  2694][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/jlqfjykhee\n",
      "[  2695][0.48 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/iztglppzcn\n",
      "[  2696][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/vudqhjbnse\n",
      "[  2697][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/fpfljuvbxj\n",
      "[  2698][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/bvtltnsjdy\n",
      "[  2699][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/uzqwcknfrc\n",
      "[  2700][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/voizaxdjnz\n",
      "[  2701][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/xdxpdhcqdf\n",
      "[  2702][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/djjdzsivvi\n",
      "[  2703][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/rpfculimra\n",
      "[  2704][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/bdbofdzikv\n",
      "[  2705][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/mhplyirnlu\n",
      "[  2706][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/ikwlskwwyf\n",
      "[  2707][0.45 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/zwwiltbegv\n",
      "[  2708][0.47 s] /home/dmitry/projects/dfdc/data/dfdc-crops/dfdc_train_part_49/tfsbglocei\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "prepare_data(start=2650, end=None, max_open_files=50, bs=30, verbose=True, \n",
    "             chunk_dirs=['dfdc_train_part_49'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
