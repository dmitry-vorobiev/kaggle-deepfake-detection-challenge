{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from numba import jit, njit\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "# SAVE_DIR = os.path.join(BASE_DIR, 'data/dfdc-crops')\n",
    "SAVE_DIR = '/media/dmitry/data/hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import ops\n",
    "\n",
    "import nvidia.dali as dali\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "# src\n",
    "sys.path.insert(0, SRC_DIR)\n",
    "from sample.reader import VideoReader\n",
    "from dataset.utils import read_labels\n",
    "\n",
    "# Pytorch_Retinaface\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'Pytorch_Retinaface'))\n",
    "from data import cfg_mnet\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from models.retinaface import RetinaFace\n",
    "from detect_utils import detect, load_model, postproc_detections, postproc_frame\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_axis(c0, c1, pad, cmax):\n",
    "    c0 = max(0, c0 - pad)\n",
    "    c1 = min(cmax, c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "@njit\n",
    "def expand_bbox(bbox, pct):\n",
    "    bbox = np.copy(bbox)\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@njit\n",
    "def crop_face(img, bbox, pad_pct=0.05):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    \n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox(bbox, pad_pct)\n",
    "        \n",
    "    x0, y0, x1, y1 = bbox.astype(np.int16)\n",
    "    \n",
    "    w, h = x1 - x0, y1 - y0\n",
    "    if w > h:\n",
    "        pad = (w - h) // 2\n",
    "        y0, y1, h = calc_axis(y0, y1, pad, img_h)\n",
    "    elif h > w:\n",
    "        pad = (h - w) // 2\n",
    "        x0, x1, w = calc_axis(x0, x1, pad, img_w)\n",
    "    \n",
    "    size = min(w, h)\n",
    "    face = img[y0:y1, x0:x1][:size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_num_faces(num_faces, frac_thresh=0.25):\n",
    "    avg = num_faces.mean()\n",
    "    fraction, integral = np.modf(avg)\n",
    "    rounded = integral if fraction < frac_thresh else integral + 1\n",
    "    return int(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPipe(dali.pipeline.Pipeline):\n",
    "    def __init__(self, file_list: str, seq_len=30, stride=10, \n",
    "                 batch_size=1, num_threads=1, device_id=0):\n",
    "        super(VideoPipe, self).__init__(\n",
    "            batch_size, num_threads, device_id, seed=3)\n",
    "        \n",
    "        self.input = dali.ops.VideoReader(\n",
    "            device='gpu', file_list=file_list, sequence_length=seq_len, \n",
    "            stride=stride, shard_id=0, num_shards=1)\n",
    "\n",
    "    def define_graph(self):\n",
    "        output, labels = self.input(name='reader')\n",
    "        return output, labels\n",
    "    \n",
    "    \n",
    "def get_file_list(df: pd.DataFrame, start: int, end: int, \n",
    "                  base_dir: str =DATA_DIR) -> List[str]:\n",
    "    path_fn = lambda row: os.path.join(base_dir, row.dir, row.name)\n",
    "    return df.iloc[start:end].apply(path_fn, axis=1).values.tolist()\n",
    "\n",
    "\n",
    "def write_file_list(files: List[str], path='./file_list.txt') -> None:    \n",
    "    with open(path, mode='w') as h:\n",
    "        for i, f in enumerate(files):\n",
    "            h.write(f'{f} {i}\\n')\n",
    "\n",
    "\n",
    "def init_detector(cfg, weights, use_cpu=False):\n",
    "    cfg['pretrain'] = False\n",
    "    net = RetinaFace(cfg=cfg, phase='test')\n",
    "    net = load_model(net, weights, use_cpu)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def mkdirs(base_dir, chunk_dirs):\n",
    "    for chunk_dir in chunk_dirs:\n",
    "        dir_path = os.path.join(base_dir, chunk_dir)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imgs(sample: Union[np.ndarray, Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "    n, h, w, c = sample.shape\n",
    "    mean = [104, 117, 123]\n",
    "    if isinstance(sample, Tensor):\n",
    "        imgs = sample.float()\n",
    "        imgs -= torch.tensor(mean, device=imgs.device)\n",
    "        imgs = imgs.permute(0, 3, 1, 2)\n",
    "    else:\n",
    "        imgs = np.float32(sample)\n",
    "        imgs -= mean\n",
    "        imgs = imgs.transpose(0, 3, 1, 2)\n",
    "        imgs = torch.from_numpy(imgs)\n",
    "    scale = torch.tensor([w, h, w, h])\n",
    "    return imgs, scale\n",
    "\n",
    "\n",
    "def detect(sample: Union[np.ndarray, Tensor], model: torch.nn.Module, \n",
    "           cfg: Dict[str,any], device: torch.device) -> List[np.ndarray]:\n",
    "    bs = cfg['batch_size']\n",
    "    num_frames, height, width, ch = sample.shape\n",
    "    imgs, scale = prepare_imgs(sample)\n",
    "    \n",
    "    priorbox = PriorBox(cfg, image_size=(height, width))\n",
    "    priors = priorbox.forward().to(device)\n",
    "    scale = scale.to(device)\n",
    "    detections = []\n",
    "    \n",
    "    for start in range(0, num_frames, bs):\n",
    "        end = start + bs\n",
    "        imgs_batch = imgs[start:end].to(device)\n",
    "        with torch.no_grad():\n",
    "            loc, conf, landms = model(imgs_batch)\n",
    "        imgs_batch, landms = None, None\n",
    "        dets = postproc_detections(loc, conf, priors, scale, cfg)\n",
    "        detections.extend(dets)\n",
    "        loc, conf = None, None\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_cv2(path: str, num_frames: int, jitter=0, seed=None) -> np.ndarray:\n",
    "    \"\"\"Reads frames that are always evenly spaced throughout the video.\n",
    "\n",
    "    Arguments:\n",
    "        path: the video file\n",
    "        num_frames: how many frames to read, -1 means the entire video\n",
    "            (warning: this will take up a lot of memory!)\n",
    "        jitter: if not 0, adds small random offsets to the frame indices;\n",
    "            this is useful so we don't always land on even or odd frames\n",
    "        seed: random seed for jittering; if you set this to a fixed value,\n",
    "            you probably want to set it only on the first video \n",
    "    \"\"\"\n",
    "    capture = cv2.VideoCapture(path)\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count <= 0: \n",
    "        return None\n",
    "    frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=np.int)\n",
    "    if jitter > 0:\n",
    "        np.random.seed(seed)\n",
    "        jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
    "        frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
    "    result = read_frames_at_indices(path, capture, frame_idxs)\n",
    "    capture.release()\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_frames_at_indices(\n",
    "        path: str, capture: cv2.VideoCapture, frame_idxs: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        frames = []\n",
    "        next_idx = 0\n",
    "        for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
    "            ret = capture.grab()\n",
    "            if not ret:\n",
    "                print('Unable to grab frame %d from %s' % (frame_idx, path))\n",
    "                break\n",
    "            if frame_idx == frame_idxs[next_idx]:\n",
    "                ret, frame = capture.retrieve()\n",
    "                if not ret or frame is None:\n",
    "                    print('Unable to retrieve frame %d from %s' % (frame_idx, path))\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "                next_idx += 1\n",
    "        if len(frames) > 0:\n",
    "            return np.stack(frames)\n",
    "        else:\n",
    "            print('No frames have been read from %s' % path)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print('Unable to read %s' % path)\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "hdf5_defaults = dict(compression=6, shuffle=True)\n",
    "\n",
    "\n",
    "def write_npz(path: str, images: List[np.ndarray]) -> None:\n",
    "    file_map = {str(i):cv2.imencode('.png', image)[1] \n",
    "                for i,image in enumerate(images)}\n",
    "    np.savez_compressed(path, **file_map)\n",
    "    \n",
    "    \n",
    "def write_hdf5(path: str, images: List[np.ndarray], \n",
    "                     append=False,\n",
    "                     opts: dict = hdf5_defaults) -> None:\n",
    "    mode = 'a' if append else 'w'\n",
    "    with h5py.File(path, mode) as file:\n",
    "        offset = len(file) if append else 0\n",
    "        for i, image in enumerate(images):\n",
    "            dataset = file.create_dataset(\n",
    "                '%03d' % (i + offset), \n",
    "                data=cv2.imencode('.png', image)[1], **opts)\n",
    "    \n",
    "    \n",
    "def read_npz(path: str) -> List[np.ndarray]:\n",
    "    images = []\n",
    "    with np.load(path) as archive:\n",
    "        for key in archive.keys():\n",
    "            img = cv2.imdecode(archive[key], cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_hdf5(path: str, num_frames=30) -> List[np.ndarray]:\n",
    "    images = []\n",
    "    with h5py.File(path, 'r+') as file:\n",
    "        total_frames = len(file)\n",
    "        pick = create_mask(num_frames, total_frames)\n",
    "        for i, key in enumerate(file.keys()):\n",
    "            if pick[i]:\n",
    "                img = np.uint8(file[key])\n",
    "                img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def calc_idxs(n: int, total: int) -> np.ndarray:\n",
    "    idxs = np.linspace(0, total, n, dtype=int, endpoint=False)\n",
    "    rnd_shift = np.random.randint(0, (total - idxs[-1]))\n",
    "    return idxs + rnd_shift\n",
    "\n",
    "\n",
    "def create_mask(n: int, total: int) -> np.ndarray:\n",
    "    mask = np.zeros(total, dtype=np.bool)\n",
    "    idxs = calc_idxs(n, total)\n",
    "    mask[idxs] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def frames_to_faces(frames: np.ndarray, detect_fn) -> None:\n",
    "    detections = detect_fn(frames)\n",
    "    if isinstance(frames, Tensor):\n",
    "        frames = frames.cpu().numpy()\n",
    "    num_faces = np.array(list(map(len, detections)), dtype=np.uint8)\n",
    "    max_faces_per_frame = round_num_faces(num_faces, frac_thresh=0.25)\n",
    "    faces = []\n",
    "    for f in range(len(frames)):\n",
    "        for det in detections[f][:max_faces_per_frame]:\n",
    "            face = crop_face(frames[f], det[:4])\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "            faces.append(face)\n",
    "    detections = None\n",
    "    return faces\n",
    "\n",
    "\n",
    "def dump_to_disk(images: List[np.ndarray], \n",
    "                 dir_path: str, filename: str, append=False) -> None:\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    file_path = os.path.join(dir_path, filename+'.h5')\n",
    "    # write_hdf5(file_path, images, append=append)\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "        start=0, end=None, chunk_dirs=None, max_open_files=100, \n",
    "        file_list_path='./temp_files.txt', verbose=False,\n",
    "        num_frames=30, stride=10, num_pass=1, \n",
    "        use_cpu=False, batch_size=32, \n",
    "        base_dir=BASE_DIR, data_dir=DATA_DIR, save_dir=SAVE_DIR):\n",
    "    df = read_labels(data_dir, chunk_dirs=chunk_dirs)\n",
    "    mkdirs(save_dir, df['dir'].unique())\n",
    "    \n",
    "    device = torch.device(\"cpu\" if use_cpu else \"cuda\")\n",
    "    weights_mnet = os.path.join(base_dir, 'data/weights/mobilenet0.25_Final.pth')\n",
    "    cfg = {**cfg_mnet, 'batch_size': batch_size}\n",
    "    detector = init_detector(cfg, weights_mnet, use_cpu).to(device)\n",
    "    detect_fn = partial(detect, model=detector, cfg=cfg, device=device)\n",
    "    \n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "    seq_len = num_frames // num_pass\n",
    "    print('Max sequences per sample: %d' % num_pass)\n",
    "    \n",
    "    for start_pos in range(start, end, max_open_files):\n",
    "        end_pos = min(start_pos + max_open_files, end)\n",
    "        files = get_file_list(df, start_pos, end_pos)\n",
    "        write_file_list(files, path=file_list_path)\n",
    "        pipe = VideoPipe(file_list_path, seq_len=seq_len, stride=stride)\n",
    "        pipe.build()\n",
    "        num_samples = len(files)\n",
    "        num_samples_read = pipe.epoch_size('reader')\n",
    "        num_bad_samples = num_samples - num_samples_read / num_pass\n",
    "        run_fallback_reader = num_bad_samples > 0\n",
    "        if run_fallback_reader:\n",
    "            proc_file_idxs = np.zeros(num_samples, dtype=np.bool)\n",
    "            \n",
    "        if num_samples_read > 0:\n",
    "            data_iter = DALIGenericIterator(\n",
    "                [pipe], ['frames', 'label'], num_samples_read, dynamic_shape=True)\n",
    "            if verbose: \n",
    "                t0 = time.time()\n",
    "            prev_idx = None\n",
    "            for video_batch in tqdm(data_iter, total=num_samples_read):\n",
    "                frames = video_batch[0]['frames'].squeeze(0)\n",
    "                read_idx =  video_batch[0]['label'].item()\n",
    "                abs_idx = start_pos + read_idx\n",
    "                meta = df.iloc[abs_idx]\n",
    "                faces = frames_to_faces(frames, detect_fn)\n",
    "                video_batch, frames = None, None\n",
    "                \n",
    "                dir_path = os.path.join(save_dir, meta.dir)\n",
    "                append = prev_idx == read_idx\n",
    "                dump_to_disk(faces, dir_path, meta.name[:-4], append=append)\n",
    "                prev_idx = read_idx\n",
    "                if run_fallback_reader:\n",
    "                    proc_file_idxs[read_idx] = True\n",
    "                if verbose:\n",
    "                    t1 = time.time()\n",
    "                    print('[%6d][%.02f s] %s/%s' % (abs_idx, t1 - t0, meta.dir, meta.name))\n",
    "                    t0 = t1\n",
    "                    \n",
    "        pipe, data_iter = None, None\n",
    "        gc.collect()\n",
    "        \n",
    "        if run_fallback_reader:\n",
    "            unproc_file_idxs = (~proc_file_idxs).nonzero()[0]\n",
    "            num_bad_samples = len(unproc_file_idxs)\n",
    "            if not num_bad_samples:\n",
    "                continue\n",
    "            print('Unable to parse %d videos with DALI' % num_bad_samples)\n",
    "            print('Running fallback decoding through OpenCV...')\n",
    "            for idx in tqdm(unproc_file_idxs):\n",
    "                if verbose: \n",
    "                    t0 = time.time()\n",
    "                frames = read_frames_cv2(files[idx], num_frames)\n",
    "                abs_idx = start_pos + idx\n",
    "                meta = df.iloc[abs_idx]\n",
    "                faces = frames_to_faces(frames, detect_fn)\n",
    "                dump_to_disk(faces, os.path.join(save_dir, meta.dir), meta.name[:-4])\n",
    "                if verbose:\n",
    "                    t1 = time.time()\n",
    "                    print('[%6d][%.02f s] %s/%s' % (abs_idx, t1 - t0, meta.dir, meta.name))\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/dmitry/projects/dfdc/data/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n",
      "Max sequences per sample: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48011c71007149a5a7cbf738466cbe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0][0.89 s] dfdc_train_part_2/qyyebirxwe.mp4\n",
      "[     1][0.44 s] dfdc_train_part_2/ntjlknlcvn.mp4\n",
      "[     2][0.56 s] dfdc_train_part_2/qivpypiwlp.mp4\n",
      "[     3][0.73 s] dfdc_train_part_2/lpkgabskbw.mp4\n",
      "[     4][0.74 s] dfdc_train_part_2/vctemjbusz.mp4\n",
      "[     5][0.71 s] dfdc_train_part_2/kligjmklcw.mp4\n",
      "[     6][0.72 s] dfdc_train_part_2/jtteetuqtt.mp4\n",
      "[     7][0.72 s] dfdc_train_part_2/bbuebaojot.mp4\n",
      "[     8][0.74 s] dfdc_train_part_2/jsnkotxoua.mp4\n",
      "[     9][0.70 s] dfdc_train_part_2/uazbhwyysx.mp4\n",
      "[    10][0.69 s] dfdc_train_part_2/fqhovztzbp.mp4\n",
      "[    11][0.73 s] dfdc_train_part_2/litakpzghb.mp4\n",
      "[    12][0.70 s] dfdc_train_part_2/bkvujfissu.mp4\n",
      "[    13][0.73 s] dfdc_train_part_2/gvleesmgxz.mp4\n",
      "[    14][0.73 s] dfdc_train_part_2/fanibwbmoq.mp4\n",
      "[    15][0.73 s] dfdc_train_part_2/gjlrssvqkt.mp4\n",
      "[    16][0.72 s] dfdc_train_part_2/smvyymdsdc.mp4\n",
      "[    17][0.71 s] dfdc_train_part_2/murqgmxknx.mp4\n",
      "[    18][0.71 s] dfdc_train_part_2/vdzlykljaf.mp4\n",
      "[    19][0.73 s] dfdc_train_part_2/sjkfxrlxxs.mp4\n",
      "[    20][0.71 s] dfdc_train_part_2/ytddugrwph.mp4\n",
      "[    21][0.73 s] dfdc_train_part_2/xauhzmhqdk.mp4\n",
      "[    22][0.69 s] dfdc_train_part_2/zsnnktadbl.mp4\n",
      "[    23][0.70 s] dfdc_train_part_2/ukclebahfm.mp4\n",
      "[    24][0.67 s] dfdc_train_part_2/xmgdvugizi.mp4\n",
      "[    25][0.74 s] dfdc_train_part_2/aqtvsxelrj.mp4\n",
      "[    26][0.74 s] dfdc_train_part_2/frcsdlkxws.mp4\n",
      "[    27][0.70 s] dfdc_train_part_2/wkxouifkzc.mp4\n",
      "[    28][0.73 s] dfdc_train_part_2/botswdcyee.mp4\n",
      "[    29][0.69 s] dfdc_train_part_2/ktkjmqrvxt.mp4\n",
      "[    30][0.74 s] dfdc_train_part_2/vbhsyoigsu.mp4\n",
      "[    31][0.69 s] dfdc_train_part_2/yyfdhmrfgg.mp4\n",
      "[    32][0.72 s] dfdc_train_part_2/iygqtvkrwf.mp4\n",
      "[    33][0.70 s] dfdc_train_part_2/lydxafrxsy.mp4\n",
      "[    34][0.71 s] dfdc_train_part_2/unmpiwboha.mp4\n",
      "[    35][0.72 s] dfdc_train_part_2/nlntwnqrzj.mp4\n",
      "[    36][0.70 s] dfdc_train_part_2/mnowxangqx.mp4\n",
      "[    37][0.72 s] dfdc_train_part_2/ihfemhgyyk.mp4\n",
      "[    38][0.73 s] dfdc_train_part_2/yiykshcbaz.mp4\n",
      "[    39][0.72 s] dfdc_train_part_2/fuqflmbxwh.mp4\n",
      "[    40][0.72 s] dfdc_train_part_2/jzsdlsaodc.mp4\n",
      "[    41][0.71 s] dfdc_train_part_2/kemtbmrfkf.mp4\n",
      "[    42][0.71 s] dfdc_train_part_2/evysmtpnrf.mp4\n",
      "[    43][0.72 s] dfdc_train_part_2/upswpfarqz.mp4\n",
      "[    44][0.72 s] dfdc_train_part_2/hgfcupavaj.mp4\n",
      "[    45][0.73 s] dfdc_train_part_2/xkkdsndlpr.mp4\n",
      "[    46][0.73 s] dfdc_train_part_2/zgjosltkie.mp4\n",
      "[    47][0.71 s] dfdc_train_part_2/xrltimmbyc.mp4\n",
      "[    48][0.71 s] dfdc_train_part_2/todbwfbpxe.mp4\n",
      "[    49][0.70 s] dfdc_train_part_2/qsjiypnjwi.mp4\n",
      "[    50][0.73 s] dfdc_train_part_2/aktnlyqpah.mp4\n",
      "[    51][0.73 s] dfdc_train_part_2/vjujyhfhrz.mp4\n",
      "[    52][0.73 s] dfdc_train_part_2/xaarfgflox.mp4\n",
      "[    53][0.73 s] dfdc_train_part_2/fdyzixbbni.mp4\n",
      "[    54][0.69 s] dfdc_train_part_2/cgigqncgru.mp4\n",
      "[    55][0.69 s] dfdc_train_part_2/hqvkabpedg.mp4\n",
      "[    56][0.73 s] dfdc_train_part_2/xgbrtjtiuy.mp4\n",
      "[    57][0.71 s] dfdc_train_part_2/kdhdcxbeti.mp4\n",
      "[    58][0.73 s] dfdc_train_part_2/akjvglpobj.mp4\n",
      "[    59][0.72 s] dfdc_train_part_2/stgweqepva.mp4\n",
      "[    60][0.72 s] dfdc_train_part_2/rznhsemash.mp4\n",
      "[    61][0.71 s] dfdc_train_part_2/yrcwgrgeoa.mp4\n",
      "[    62][0.72 s] dfdc_train_part_2/fmegtxqovc.mp4\n",
      "[    63][0.73 s] dfdc_train_part_2/atophdvemg.mp4\n",
      "[    64][0.75 s] dfdc_train_part_2/daduxgwpjz.mp4\n",
      "[    65][0.70 s] dfdc_train_part_2/wrmypaurck.mp4\n",
      "[    66][0.72 s] dfdc_train_part_2/aabdnomlru.mp4\n",
      "[    67][0.71 s] dfdc_train_part_2/hhxaxylkgv.mp4\n",
      "[    68][0.71 s] dfdc_train_part_2/esjnbixlal.mp4\n",
      "[    69][0.74 s] dfdc_train_part_2/psjvaeamke.mp4\n",
      "[    70][0.73 s] dfdc_train_part_2/dktpniyvec.mp4\n",
      "[    71][0.75 s] dfdc_train_part_2/wruglpwldh.mp4\n",
      "[    72][0.69 s] dfdc_train_part_2/izqloinjwk.mp4\n",
      "[    73][0.72 s] dfdc_train_part_2/fcbwiaddld.mp4\n",
      "[    74][0.75 s] dfdc_train_part_2/rdvvkfvtnm.mp4\n",
      "[    75][0.72 s] dfdc_train_part_2/doeeuppytb.mp4\n",
      "[    76][0.74 s] dfdc_train_part_2/rllpgflwaq.mp4\n",
      "[    77][0.77 s] dfdc_train_part_2/guclsnyxvq.mp4\n",
      "[    78][0.71 s] dfdc_train_part_2/ucnnsarnao.mp4\n",
      "[    79][0.72 s] dfdc_train_part_2/hbmumtptii.mp4\n",
      "[    80][0.74 s] dfdc_train_part_2/ztyvglkcsf.mp4\n",
      "[    81][0.71 s] dfdc_train_part_2/agziimvuqi.mp4\n",
      "[    82][0.71 s] dfdc_train_part_2/mmgvyvdaqt.mp4\n",
      "[    83][0.73 s] dfdc_train_part_2/vyxryahfuw.mp4\n",
      "[    84][0.92 s] dfdc_train_part_2/sljaizccqh.mp4\n",
      "[    85][0.55 s] dfdc_train_part_2/pyriecynrt.mp4\n",
      "[    86][0.75 s] dfdc_train_part_2/anuzdiqqnq.mp4\n",
      "[    87][0.70 s] dfdc_train_part_2/kpvvdydcev.mp4\n",
      "[    88][0.71 s] dfdc_train_part_2/megxyffexb.mp4\n",
      "[    89][0.73 s] dfdc_train_part_2/condbpyazo.mp4\n",
      "[    90][0.71 s] dfdc_train_part_2/cwasnpqxck.mp4\n",
      "[    91][0.73 s] dfdc_train_part_2/uatziuqdfx.mp4\n",
      "[    92][0.74 s] dfdc_train_part_2/ctwqyniuuk.mp4\n",
      "[    93][0.72 s] dfdc_train_part_2/nnkvgtmecd.mp4\n",
      "[    94][0.74 s] dfdc_train_part_2/xdxmcxbyjz.mp4\n",
      "[    95][0.74 s] dfdc_train_part_2/rtmbucxfef.mp4\n",
      "[    96][0.74 s] dfdc_train_part_2/fglewmddcn.mp4\n",
      "[    97][0.70 s] dfdc_train_part_2/wywmsnneuv.mp4\n",
      "[    98][0.73 s] dfdc_train_part_2/wydfrjeniu.mp4\n",
      "[    99][0.70 s] dfdc_train_part_2/wjqqxxabcv.mp4\n",
      "\n",
      "DONE\n",
      "CPU times: user 50.9 s, sys: 13.4 s, total: 1min 4s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "prepare_data(start=0, end=100, max_open_files=300, batch_size=32, \n",
    "             num_frames=30, stride=10, num_pass=1,\n",
    "             verbose=True, chunk_dirs=['dfdc_train_part_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_images(images, cols = 1, titles = None):\n",
    "#     \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "#     Parameters\n",
    "#     ---------\n",
    "#     images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "#     cols (Default = 1): Number of columns in figure (number of rows is \n",
    "#                         set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "#     titles: List of titles corresponding to each image. Must have\n",
    "#             the same length as titles.\n",
    "#     \"\"\"\n",
    "#     assert((titles is None)or (len(images) == len(titles)))\n",
    "#     n_images = len(images)\n",
    "#     if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "#     fig = plt.figure()\n",
    "#     for n, (image, title) in enumerate(zip(images, titles)):\n",
    "#         a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "#         if image.ndim == 2:\n",
    "#             plt.gray()\n",
    "#         plt.imshow(image)\n",
    "#         a.set_title(title)\n",
    "#     fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/media/dmitry/data/hdf5/dfdc_train_part_1/fjzrvkleur.h5'\n",
    "# num_frames = 30\n",
    "# cols = math.ceil(num_frames / 6)\n",
    "# faces = read_hdf5(path, num_frames=num_frames)        \n",
    "# show_images(faces, cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_len_hdf5(path):\n",
    "#     lens = dict()\n",
    "#     for name in os.listdir(path):\n",
    "#         full_path = os.path.join(path, name)\n",
    "#         if os.path.isfile(full_path):\n",
    "#             with h5py.File(full_path, 'r+') as f:\n",
    "#                 lens[name] = len(f)\n",
    "#     return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_len_hdf5('/media/dmitry/data/hdf5/dfdc_train_part_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/media/dmitry/data/dfdc_train_part_2/jsnkotxoua.h5.npz'\n",
    "# faces = load_images_from_npz(path)        \n",
    "# show_images(faces, cols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bench(path, read_fn):\n",
    "#     for name in os.listdir(path):\n",
    "#         full_path = os.path.join(path, name)\n",
    "#         if os.path.isfile(full_path):\n",
    "#             read_fn(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bench('/media/dmitry/data/npz/dfdc_train_part_2', load_images_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bench('/media/dmitry/data/hdf5/dfdc_train_part_2', \n",
    "#       partial(load_images_hdf5, num_frames=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bench('/home/dmitry/projects/dfdc/data/npz/dfdc_train_part_2', load_images_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bench('/home/dmitry/projects/dfdc/data/hdf5/dfdc_train_part_2', \n",
    "#       partial(load_images_hdf5, num_frames=30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
