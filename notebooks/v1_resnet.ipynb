{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kb/df_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.3.1\n",
      "Torchvision Version:  0.4.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "\n",
    "is_alchemy_used = True\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from skimage import io, transform\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Dataset\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "if is_alchemy_used:\n",
    "    from catalyst.dl import SupervisedAlchemyRunner as SupervisedRunner\n",
    "else:\n",
    "    from catalyst.dl import SupervisedRunner\n",
    "\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "from albumentations import Compose, RandomCrop, Normalize, HorizontalFlip, Resize, RandomResizedCrop\n",
    "from albumentations.pytorch import ToTensor\n",
    "from alchemy import Logger\n",
    "token = \"d1dd16f08d518293bcbeddd313b49aa4\"\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "def seed_everything(seed=12345):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.3.1, catalyst: 20.02.3\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List, Tuple \n",
    "\n",
    "import os\n",
    "import torch\n",
    "import catalyst\n",
    "\n",
    "from catalyst.dl import utils\n",
    "\n",
    "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "\n",
    "SEED = 42\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = f'/home/{os.environ[\"USER\"]}/projects/dfdc/'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/dfdc-videos')\n",
    "HDF5_DIR = f'/home/{os.environ[\"USER\"]}/projects/dfdc/data/dfdc-crops/hdf5'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 16#32\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 10\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "def my_initialize_model(file_checkpoint, model_name, feature_extract, emb_len):\n",
    "    \n",
    "\n",
    "    model, input_size = initialize_model(model_name, 2, feature_extract, use_pretrained=True)\n",
    "#     model = model.to(device)\n",
    "    if file_checkpoint != None:\n",
    "        checkpoint = torch.load(file_checkpoint)#, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    _ = model.eval()\n",
    "    \n",
    "    if file_checkpoint != None:\n",
    "        del checkpoint\n",
    "\n",
    "#     emb_len = 128\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, emb_len)\n",
    "    return model, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from network.models import model_selection\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_name, emb_len, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.backbone, self.input_size = my_initialize_model(None, model_name, False, emb_len)\n",
    "        self.lstm = nn.LSTM(emb_len, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, 2)\n",
    "#         self.out2tag = nn.Linear(self.input_size, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, sentences):\n",
    "        self.lstm.flatten_parameters()\n",
    "        \n",
    "        tag_scores_list = torch.zeros((sentences.shape[0], 2), dtype=torch.float32 ).cuda()\n",
    "#         print(tag_scores_list.shape)\n",
    "        for i, sentence in enumerate(sentences):   \n",
    "            embeds = self.backbone(sentence.permute(0, 3, 1, 2))\n",
    "#             print(embeds.shape)\n",
    "            lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "#             print(lstm_out.shape)\n",
    "            tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))     \n",
    "#             print(tag_space.shape)\n",
    "            tag_scores_list[i] = tag_space[-1,:]\n",
    "        return tag_scores_list\n",
    "\n",
    "\n",
    "model = Net('resnet', 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Iterator, List, Optional, Tuple, Union\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(BASE_DIR, 'src'))\n",
    "from dataset.utils import read_labels\n",
    "from prepare_data import get_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len_hdf5(path):\n",
    "    lens = dict()\n",
    "    for name in os.listdir(path):\n",
    "        full_path = os.path.join(path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            with h5py.File(full_path, 'r+') as f:\n",
    "                lens[name] = len(f)\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_frames(df, idxs):\n",
    "    lens = []\n",
    "    for idx in idxs:\n",
    "        meta = df.iloc[idx]\n",
    "        path = os.path.join(HDF5_DIR, meta.dir, meta.name[:-4]+'.h5')\n",
    "        if os.path.isfile(path):\n",
    "            with h5py.File(path, 'r+') as f:\n",
    "                lens.append(len(f))\n",
    "        else:\n",
    "            lens.append(-1)\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5(path: str, num_frames: int, size: int,\n",
    "              sample_fn: Callable[[int], np.ndarray]) -> np.ndarray:\n",
    "    img_size = (size, size)\n",
    "    images = []\n",
    "    with h5py.File(path, 'r+') as file:\n",
    "        total_frames = len(file)\n",
    "        if total_frames > 0:\n",
    "            idxs = sample_fn(total_frames)\n",
    "            pick = create_mask(idxs, total_frames)\n",
    "            for i, key in enumerate(file.keys()):\n",
    "                if pick[i]:\n",
    "                    img = np.uint8(file[key])\n",
    "                    img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, img_size, \n",
    "                                     interpolation=cv2.INTER_NEAREST)\n",
    "                    images.append(img)\n",
    "            return np.stack(images)\n",
    "        else:\n",
    "            return np.empty((0, size, size, 3), dtype=np.uint8)\n",
    "        \n",
    "        \n",
    "def sparse_frames(n: int, total: int) -> np.ndarray:\n",
    "    idxs = np.linspace(0, total, min(n, total), dtype=int, endpoint=False)\n",
    "    rnd_shift = np.random.randint(0, (total - idxs[-1]))\n",
    "    return idxs + rnd_shift\n",
    "\n",
    "\n",
    "def rnd_slice_frames(n: int, total: int, stride=1.) -> np.ndarray:\n",
    "    idxs = np.arange(0, total, stride)[:n].astype(np.uint16)\n",
    "    rnd_shift = np.random.randint(0, (total - idxs[-1]))\n",
    "    return idxs + rnd_shift\n",
    "\n",
    "\n",
    "def create_mask(idxs: np.ndarray, total: int) -> np.ndarray:\n",
    "    mask = np.zeros(total, dtype=np.bool)\n",
    "    mask[idxs] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def pad(frames: np.ndarray, amount: int, where :str='start') -> np.ndarray:\n",
    "    dims = np.zeros((frames.ndim, 2), dtype=np.int8)\n",
    "    pad_dim = 1 if where == 'end' else 0\n",
    "    dims[0, pad_dim] = amount\n",
    "    return np.pad(frames, dims, 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSampler():\n",
    "    def __init__(self, num_frames: int, real_fake_ratio: float, \n",
    "                 p_sparse: float):\n",
    "        self.num_frames = num_frames\n",
    "        self.real_fake_ratio = real_fake_ratio\n",
    "        self.p_sparse = p_sparse\n",
    "        \n",
    "    def __call__(self, label: Tuple[int, bool]) -> Callable[[int], np.ndarray]:\n",
    "        dice = np.random.rand()\n",
    "        if dice < self.p_sparse:\n",
    "            return partial(sparse_frames, self.num_frames)\n",
    "        else:\n",
    "            # Stored frames: fake - 30, real - 150, \n",
    "            # the real_fake_ratio should be set to 150 / 30 = 5\n",
    "            # stride for fake: 5 - (4 * 1) = 1\n",
    "            # stride for real: 5 - (4 * 0) = 5\n",
    "            n = self.real_fake_ratio\n",
    "            stride = n - ((n-1) * int(label))\n",
    "            return partial(rnd_slice_frames, self.num_frames, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = FrameSampler(num_frames=15, real_fake_ratio=30/30, p_sparse=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path: str, hdf5_path: str, \n",
    "                 size: Tuple[int, int], \n",
    "                 sampler: FrameSampler):\n",
    "        self.df = VideoDataset._read_annotations(data_path, hdf5_path)\n",
    "        self.size = size\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    @staticmethod\n",
    "    def _read_annotations(data_path: str, hdf5_path: str) -> pd.DataFrame:\n",
    "        if not os.path.isdir(data_path):\n",
    "            raise RuntimeError('Invalid data dir.. Wake up your drive brah!')\n",
    "        parts = []\n",
    "        for chunk_dir in os.listdir(data_path):\n",
    "            hdf5_chunk_path = Path(hdf5_path)/chunk_dir\n",
    "            if not hdf5_chunk_path.is_dir():\n",
    "                print('{}: dir is missing..'.format(hdf5_chunk_path))\n",
    "                continue\n",
    "            meta_path = Path(data_path)/chunk_dir/'metadata.json'\n",
    "            df = pd.read_json(meta_path).T\n",
    "            df = df.reset_index().rename({'index': 'file'}, axis=1)\n",
    "            df['label'] = df['label'] == 'FAKE'\n",
    "            df['file'] = df['file'].apply(lambda file: file[:-4]+'.h5')\n",
    "            df['dir'] = chunk_dir\n",
    "            df['missing'] = False\n",
    "            for i in range(len(df)):\n",
    "                hdf5_file = df.loc[i, 'file']\n",
    "                path = hdf5_chunk_path/hdf5_file\n",
    "                if not path.is_file():\n",
    "                    df.loc[i, 'missing'] = True\n",
    "            num_miss = df['missing'].sum()\n",
    "            if num_miss > 0:\n",
    "                print('{}: {} files missing..'.format(\n",
    "                    hdf5_chunk_path, num_miss))\n",
    "                df = df[~df['missing']]\n",
    "            df.drop(['split', 'missing'], axis=1, inplace=True)\n",
    "            parts.append(df)\n",
    "        return pd.concat(parts)\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[np.ndarray, int]:\n",
    "        num_frames, size = self.size\n",
    "        meta = self.df.iloc[idx]\n",
    "        label = int(meta.label)\n",
    "        path = os.path.join(self.hdf5_path, meta.dir, meta.file)\n",
    "        \n",
    "        if os.path.isfile(path):\n",
    "            sample_fn = self.sampler(meta.label)\n",
    "            frames = read_hdf5(path, num_frames, size, sample_fn=sample_fn)\n",
    "        else:\n",
    "            print('Unable to read {}'.format(path))\n",
    "            frames = np.zeros((num_frames, size, size, 3), dtype=np.uint8)\n",
    "        \n",
    "        if len(frames) > 0:\n",
    "            pad_amount = num_frames - len(frames)\n",
    "            if pad_amount > 0:\n",
    "                frames = pad(frames, pad_amount, 'start')\n",
    "        else:\n",
    "            print('Empty file {}'.format(path))\n",
    "            frames = np.zeros((num_frames, size, size, 3), dtype=np.uint8)\n",
    "            \n",
    "        frames = np.array(frames, dtype=np.float32)\n",
    "        tr = Compose([\n",
    "            \n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#             ToTensor()\n",
    "\n",
    "        ])\n",
    "        frames =np.asarray([tr(image=frame)['image'] for frame in frames ], dtype=np.float32)\n",
    "#         print(frames.dtype)\n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffled_idxs(values: np.ndarray, val: int) -> List[int]:\n",
    "    idxs = (values == val).nonzero()[0]\n",
    "    idxs = np.random.permutation(idxs)\n",
    "    return idxs\n",
    "\n",
    "\n",
    "class BalancedSampler(torch.utils.data.RandomSampler):\n",
    "    def __init__(self, data_source, replacement=False, num_samples=None):\n",
    "        \n",
    "        super().__init__(data_source, replacement, num_samples)\n",
    "        if not hasattr(data_source, 'df'):\n",
    "            raise ValueError(\"DataSource must have a 'df' property\")\n",
    "            \n",
    "        if not 'label' in data_source.df: \n",
    "            raise ValueError(\"DataSource.df must have a 'label' column\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        df = self.data_source.df\n",
    "        all_labels = df['label'].values\n",
    "        uniq_labels, label_freq = np.unique(all_labels, return_counts=True)\n",
    "        rev_freq = (len(all_labels) / label_freq)\n",
    "        \n",
    "        idxs = []\n",
    "        for freq, label in zip(rev_freq, uniq_labels):\n",
    "            fraction, times = np.modf(freq)\n",
    "            label_idxs = (all_labels == label).nonzero()[0]\n",
    "            for _ in range(int(times)):\n",
    "                label_idxs = np.random.permutation(label_idxs)\n",
    "                idxs.append(label_idxs)\n",
    "            if fraction > 0.05:\n",
    "                label_idxs = np.random.permutation(label_idxs)\n",
    "                chunk = int(len(label_idxs) * fraction)\n",
    "                idxs.append(label_idxs[:chunk])\n",
    "        idxs = np.concatenate(idxs)\n",
    "        idxs = np.random.permutation(idxs)[:self.num_samples]\n",
    "        \n",
    "        for i in idxs:\n",
    "            yield i\n",
    "        # return iter(idxs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_46: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_41: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_32: 22 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_34: 1 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_45: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_21: 18 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_16: 1 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_18: 3 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_48: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_47: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_43: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_12: 44 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_38: 9 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_40: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_44: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_25: 4 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_27: 2 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_35: 5 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_8: 2 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_23: 16 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_36: 125 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_49: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-train/dfdc_train_part_42: dir is missing..\n",
      "94114\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_26: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_3: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_46: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_41: 3 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_39: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_32: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_34: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_28: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_45: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_2: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_17: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_21: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_16: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_31: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_20: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_18: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_22: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_7: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_48: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_19: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_10: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_6: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_24: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_1: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_47: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_33: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_37: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_43: 1 files missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_5: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_12: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_13: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_14: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_38: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_30: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_9: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_4: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_25: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_0: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_27: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_35: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_15: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_8: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_11: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_29: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_23: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_36: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-valid/dfdc_train_part_49: dir is missing..\n",
      "12233\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_26: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_3: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_41: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_39: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_32: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_34: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_28: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_2: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_17: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_21: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_16: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_31: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_20: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_18: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_22: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_7: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_19: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_10: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_6: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_24: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_1: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_33: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_37: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_43: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_5: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_12: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_13: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_14: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_38: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_30: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_9: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_40: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_44: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_4: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_25: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_0: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_27: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_35: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_15: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_8: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_11: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_29: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_23: dir is missing..\n",
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_36: dir is missing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kb/projects/dfdc/data/dfdc-crops-test/dfdc_train_part_42: dir is missing..\n",
      "12551\n"
     ]
    }
   ],
   "source": [
    "input_size = model.input_size\n",
    "FASTPART=False\n",
    "if FASTPART:\n",
    "    num_frames = 2\n",
    "else:\n",
    "    num_frames = 15\n",
    "def get_loader(num_frames=15, real_fake_ratio=1, p_sparse=0.5, input_size=input_size, hdf_dir=None):\n",
    "    \n",
    "    sampler = FrameSampler(num_frames, real_fake_ratio=real_fake_ratio, p_sparse=p_sparse)\n",
    "    ds = VideoDataset(DATA_DIR, hdf_dir, size=(num_frames, input_size), sampler=sampler)\n",
    "    print(len(ds))\n",
    "    s = BalancedSampler(ds)\n",
    "    batch_sampler = torch.utils.data.BatchSampler(\n",
    "        BalancedSampler(ds), \n",
    "        batch_size=batch_size, \n",
    "        drop_last=True\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_sampler=batch_sampler)\n",
    "    return dl\n",
    "    \n",
    "loaders = {}\n",
    "loaders['train'] = get_loader(num_frames=num_frames, real_fake_ratio=1, p_sparse=0.5, input_size=input_size, \n",
    "                              hdf_dir=f'/home/{os.environ[\"USER\"]}/projects/dfdc/data/dfdc-crops-train/')\n",
    "loaders['valid'] = get_loader(num_frames=num_frames, real_fake_ratio=1, p_sparse=0.5, input_size=input_size, \n",
    "                              hdf_dir=f'/home/{os.environ[\"USER\"]}/projects/dfdc/data/dfdc-crops-valid/')\n",
    "loaders['test'] = get_loader(num_frames=num_frames, real_fake_ratio=1, p_sparse=0.5, input_size=input_size, \n",
    "                              hdf_dir=f'/home/{os.environ[\"USER\"]}/projects/dfdc/data/dfdc-crops-test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Experiment: exp0\n",
      "1/5 * Epoch (train):   2% 106/5882 [01:37<1:34:50,  1.02it/s, loss=0.688]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "project = 'dfdc_v1_resnet'\n",
    "num_epochs = 5\n",
    "\n",
    "group = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "\n",
    "if FASTPART:\n",
    "    group = f'fast_{group}'\n",
    "    \n",
    "expnum = 0\n",
    "experiment = f\"exp{expnum}\"\n",
    "logdir = f\"/home/kb/hdd/logs/deepfake/{project}/{group}/{experiment}\"\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "else:\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            pass\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# model runner\n",
    "runner = SupervisedRunner()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'----------------Experiment: {experiment}')\n",
    "logger = Logger(\n",
    "    token=token,\n",
    "    experiment=experiment,\n",
    "    group=group,\n",
    "    project=project,\n",
    ")\n",
    "\n",
    "logger.close()\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    monitoring_params={\n",
    "        \"token\": token,\n",
    "        \"project\": project,\n",
    "        \"experiment\": experiment,\n",
    "        \"group\": group,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df_env",
   "language": "python",
   "name": "df_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
