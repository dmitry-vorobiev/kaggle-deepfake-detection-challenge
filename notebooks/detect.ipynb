{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn, Tensor, IntTensor, LongTensor, FloatTensor\n",
    "\n",
    "from functools import partial\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Callable, Dict, Iterable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')\n",
    "VID_DIR = '/media/dmitry/data/dfdc-videos/dfdc_train_part_38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, f'/home/{os.environ[\"USER\"]}/projects/dfdc/vendors/Pytorch_Retinaface')\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from models.retinaface import RetinaFace\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, SRC_DIR)\n",
    "sys.path.insert(0, './utils')\n",
    "from detectors.retinaface import init_detector, prepare_imgs, decode_batch\n",
    "from video import read_frames_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualise import show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load('../config/predict.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_detector_cfg(conf: DictConfig) -> Dict[str, any]:\n",
    "    cfg = cfg_mnet if conf.encoder == 'mnet' else cfg_re50\n",
    "    for key in \"batch_size, score_thresh, nms_thresh, top_k, keep_top_k\".split(\", \"):\n",
    "        if key not in conf or conf[key] is None:\n",
    "            raise AttributeError(\"Missing {} in detector config\".format(key))\n",
    "    cfg = {**cfg, **conf}\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from /home/dmitry/projects/dfdc/data/weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n"
     ]
    }
   ],
   "source": [
    "face_det_conf = merge_detector_cfg(conf['face-detection'])\n",
    "detector = init_detector(\n",
    "    face_det_conf, \n",
    "    face_det_conf['weights'], \n",
    "    device\n",
    ").to(device)\n",
    "# detect_fn = partial(detect, model=detector, cfg=face_det_conf, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(VID_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(VID_DIR, files[12])\n",
    "sample_np = read_frames_cv2(path, 30)\n",
    "sample_orig = torch.from_numpy(sample_np).to(device)\n",
    "\n",
    "D, H, W, C = sample_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_orig = sample_orig.permute(0, 3, 1, 2)\n",
    "\n",
    "mean = [104, 117, 123]\n",
    "    if isinstance(sample, Tensor):\n",
    "        imgs = sample.float()\n",
    "        imgs -= torch.tensor(mean, device=imgs.device)\n",
    "        imgs = imgs.permute(0, 3, 1, 2)\n",
    "        \n",
    "scale = torch.tensor([W, H, W, H])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 3, 1080, 1920]), tensor([1920, 1080, 1920, 1080]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, scale = prepare_imgs(sample_orig)\n",
    "sample.shape, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, C, H, W = sample.shape\n",
    "priorbox = PriorBox(face_det_conf, image_size=(H, W))\n",
    "priors = priorbox.forward().to(device)\n",
    "scale = scale.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    locations, confidence, landmarks = detector(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postproc_frame_torch(boxes: Tensor, scores: Tensor, conf: Dict[str, Any]) -> Tensor:\n",
    "    idxs = (scores > conf['score_thresh']).nonzero().squeeze_(1)\n",
    "    if idxs.size(0):\n",
    "        boxes = boxes[idxs]\n",
    "        scores = scores[idxs]\n",
    "\n",
    "        # keep top-K before NMS\n",
    "        top_k = conf['top_k']\n",
    "        scores, idxs = scores.sort(descending=True)\n",
    "        scores, idxs = scores[:top_k], idxs[:top_k]\n",
    "        boxes = boxes[idxs]\n",
    "\n",
    "        # do NMS\n",
    "        nms_thresh = conf['nms_thresh']\n",
    "        keep_top_k = conf['keep_top_k']\n",
    "        keep = torchvision.ops.nms(boxes, scores, nms_thresh)\n",
    "        boxes = boxes[keep][:keep_top_k]\n",
    "        scores = scores[keep][:keep_top_k]\n",
    "        scores = scores.unsqueeze_(1)\n",
    "        return torch.cat([boxes, scores], dim=1)\n",
    "    else:\n",
    "        return torch.empty(0, 5, device=boxes.device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def postproc_detections(locations: Tensor, confidence: Tensor, priors: Tensor, \n",
    "                        scale: Tensor, conf: Dict[str, any], resize=1) -> List[Tensor]:\n",
    "    boxes = decode_batch(locations, priors, conf['variance'])\n",
    "    boxes = boxes * scale / resize\n",
    "    scores = confidence[:, :, 1]\n",
    "    N = boxes.size(0)\n",
    "    out = []\n",
    "    for f in range(N):\n",
    "        boxes_f = postproc_frame_torch(boxes[f], scores[f], conf)\n",
    "        out.append(boxes_f)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets = postproc_detections(locations, confidence, priors, scale, face_det_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = 1\n",
    "boxes = decode_batch(locations, priors, face_det_conf['variance'])\n",
    "boxes = boxes * scale / resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_axis_torch(c0: IntTensor, c1: IntTensor, pad: IntTensor, \n",
    "                    cmax: int) -> Tuple[IntTensor, ...]:\n",
    "    c0 = max(0, c0 - pad)\n",
    "    c1 = min(cmax, c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "def expand_bbox_torch(bbox: FloatTensor, pct: float) -> FloatTensor:\n",
    "    bbox = bbox.clone().detach()\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def fix_coords_torch(bbox: FloatTensor, img_width: int, img_height: int) -> Tuple[FloatTensor, ...]:    \n",
    "    x0, y0, x1, y1 = bbox.int()\n",
    "    x0.clamp_min_(0)\n",
    "    y0.clamp_min_(0)\n",
    "    x1.clamp_max_(img_width)\n",
    "    y1.clamp_max_(img_height)\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "\n",
    "def crop_square_torch(img: IntTensor, bbox: FloatTensor, pad_pct=0.05) -> IntTensor:\n",
    "    C, H, W = img.shape\n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox_torch(bbox, pad_pct)\n",
    "    x0, y0, x1, y1 = fix_coords_torch(bbox, W, H)\n",
    "    w, h = x1 - x0, y1 - y0\n",
    "    if w > h:\n",
    "        pad = (w - h) // 2\n",
    "        y0, y1, h = calc_axis_torch(y0, y1, pad, H)\n",
    "    elif h > w:\n",
    "        pad = (h - w) // 2\n",
    "        x0, x1, w = calc_axis_torch(x0, x1, pad, W)\n",
    "    size = min(w, h)\n",
    "    face = img[:, y0:y1, x0:x1][:, :size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def calc_axis_torch(c0: Tensor, c1: Tensor, pad: int, \n",
    "                    cmax: int):\n",
    "    c0 = torch.max(torch.zeros_like(c0), c0 - pad)\n",
    "    c1 = torch.min(torch.full_like(c1, cmax), c1 + pad)\n",
    "    return c0, c1, c1 - c0\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def expand_bbox_torch(bbox: Tensor, pct: float) -> Tensor:\n",
    "    bbox = bbox.clone().detach()\n",
    "    bbox[:2] *= 1 - pct\n",
    "    bbox[2:] *= 1 + pct\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def fix_coords_torch(bbox: Tensor, img_width: int, img_height: int) -> Tensor:    \n",
    "    bbox = bbox.int()\n",
    "    bbox[0].clamp_min_(0)\n",
    "    bbox[1].clamp_min_(0)\n",
    "    bbox[2].clamp_max_(img_width)\n",
    "    bbox[3].clamp_max_(img_height)\n",
    "    return bbox\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def crop_square_torch(img: Tensor, bbox: Tensor, pad_pct=torch.tensor(0.05)) -> Tensor:\n",
    "    C, H, W = img.shape\n",
    "    if pad_pct > 0:\n",
    "        bbox = expand_bbox_torch(bbox, pad_pct)\n",
    "    bbox = fix_coords_torch(bbox, W, H)\n",
    "    x0, y0, x1, y1 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "    w, h = x1 - x0, y1 - y0\n",
    "    if w > h:\n",
    "        pad = (w - h) // 2\n",
    "        y0, y1, h = calc_axis_torch(y0, y1, pad, H)\n",
    "    elif h > w:\n",
    "        pad = (h - w) // 2\n",
    "        x0, x1, w = calc_axis_torch(x0, x1, pad, W)\n",
    "    size = min(w, h)\n",
    "    face = img[:, y0:y1, x0:x1][:, :size, :size]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 3, 1080, 1920]), 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape, len(dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 771 ms, sys: 13.6 ms, total: 784 ms\n",
      "Wall time: 784 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(100):\n",
    "    for f in range(27):\n",
    "        crop = crop_square_torch(sample_orig[f].permute(2, 0, 1), dets[f][0, :4])\n",
    "# print('Done')\n",
    "# print(crop.shape)\n",
    "# crop = F.interpolate(crop[None, :].float(), size=256).byte()\n",
    "# print(crop.shape)\n",
    "# crop1 = crop.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "# show_images(crop1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
