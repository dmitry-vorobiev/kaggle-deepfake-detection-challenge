{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_block(in_ch: int, out_ch: int, kernel=3, \n",
    "              stride=2, pad=0, bn=True):\n",
    "    conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel, \n",
    "                     stride=stride, padding=pad)\n",
    "    relu = nn.ReLU(inplace=True)\n",
    "    if bn:\n",
    "        layers = [conv, nn.BatchNorm2d(out_ch), relu]\n",
    "    else:\n",
    "        layers = [conv, relu]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel=3, \n",
    "                 scale=2, pad=0, bn=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = partial(F.interpolate, \n",
    "                                scale_factor=scale, \n",
    "                                mode='nearest')\n",
    "        conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel, \n",
    "                         stride=1, padding=pad)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        if bn:\n",
    "            layers = [conv, nn.BatchNorm2d(out_ch), relu]\n",
    "        else:\n",
    "            layers = [conv, relu]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_old(x: Tensor, y: Tensor) -> Tensor:\n",
    "    N, C, H, W = x.shape\n",
    "    half_C = C // 2\n",
    "\n",
    "    low  = half_C * y\n",
    "    high = half_C * (y + 1)\n",
    "\n",
    "    x = x.clone()\n",
    "    for i in range(N):\n",
    "        x[i, low[i]:high[i]] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(h: Tensor, y: Tensor) -> Tensor:\n",
    "    N, C, H, W = h.shape\n",
    "    y = y.reshape(N, 1, 1, 1)\n",
    "    \n",
    "    h0, h1 = h.chunk(2, dim=1)\n",
    "    h0 = h0 * (1 - y)\n",
    "    h1 = h1 * y\n",
    "    h = torch.cat([h0, h1], dim=1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(h: Tensor, y: Tensor) -> Tensor:\n",
    "    N, C, H, W = h.shape\n",
    "\n",
    "    y = y.reshape(N, 1, 1, 1)\n",
    "    h0, h1 = h.chunk(2, dim=1)\n",
    "    a = h0 * (1 - y) + h1 * y\n",
    "    \n",
    "    n_el = C * H * W / 2\n",
    "    a = a.abs().sum((1, 2, 3)) / n_el\n",
    "    \n",
    "    # For simplicity, and without losing generality, \n",
    "    # we constrain a(x) to be equal to 1\n",
    "    return a.clamp_max_(1).ceil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tensor([\n",
    "    [1,1,0,  1,1,0],\n",
    "    [0,0,0,  0,0,0],\n",
    "    [1,0,1,  0,1,0],\n",
    "    [1,1,1,  0,0,0],\n",
    "    [0,0,0,  1,1,1]\n",
    "], dtype=torch.float32, requires_grad=True)[:,:,None,None]\n",
    "\n",
    "y = torch.tensor([1, 0, 1, 0, 1])\n",
    "\n",
    "all_neg = torch.zeros(y.size(0), dtype=torch.int64)\n",
    "all_pos = torch.ones(y.size(0), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, all_pos).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 0., 1.], grad_fn=<CeilBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, all_neg).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 0.], grad_fn=<CeilBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, all_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, y).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 1.], grad_fn=<CeilBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{ACT} =\n",
    "\\sum_{x ∈ S_0}\n",
    "|a_0(x) − 1| + |a_1(x)| +\n",
    "\\sum_{x ∈ S_1}\n",
    "|a_1(x) − 1| + |a_0(x)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(n: int) -> Tensor:\n",
    "    return torch.zeros(n, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def ones(n: int) -> Tensor:\n",
    "    return torch.ones(n, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def act_loss(x: Tensor, y: Tensor) -> Tensor:\n",
    "    pos = y.nonzero().reshape(-1)\n",
    "    neg = (y - 1).nonzero().reshape(-1)\n",
    "    x0, x1 = x[neg], x[pos]\n",
    "    n0, n1 = x0.size(0), x1.size(0)\n",
    "    \n",
    "    a0_x0 = act(x0, zeros(n0))\n",
    "    a1_x0 = act(x0, ones(n0))\n",
    "    \n",
    "    a1_x1 = act(x1, ones(n1))\n",
    "    a0_x1 = act(x1, zeros(n1))\n",
    "    \n",
    "    neg_loss = (a0_x0 - 1).abs() + a1_x0\n",
    "    pos_loss = (a1_x1 - 1).abs() + a0_x1\n",
    "\n",
    "    return (neg_loss.sum() + pos_loss.sum()) / y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6000, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_loss(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reconstruction_loss(x: Tensor, x_hat: Tensor) -> Tensor:\n",
    "#     return (x - x_hat).abs().sum() / x.numel()\n",
    "\n",
    "\n",
    "def rec_loss(x: Tensor, x_hat: Tensor) -> Tensor:\n",
    "    return F.l1_loss(x_hat, x, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_ch: int, depth: int, size=8, pad=1):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Autoencoder._build_encoder(in_ch, depth, size, pad)\n",
    "        self.decoder = Autoencoder._build_decoder(in_ch, depth, size, pad)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_encoder(in_ch: int, depth: int, size: int, pad: int) -> nn.Module:        \n",
    "        stem = enc_block(in_ch, size, stride=1, pad=pad, bn=False)\n",
    "        main = [enc_block(size * 2**i, size * 2**(i+1), pad=pad) \n",
    "                for i in range(0, depth - 1)]\n",
    "        return nn.Sequential(stem, *main)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_decoder(out_ch: int, depth: int, size: int, pad: int) -> nn.Module:\n",
    "        main = [DecoderBlock(size * 2**(i+1), size * 2**i, pad=pad) \n",
    "                for i in sorted(range(0, depth - 1), reverse=True)]\n",
    "        last = nn.Conv2d(size, out_ch, 3, stride=1, padding=pad)\n",
    "        return nn.Sequential(*main, last, nn.Tanh())\n",
    "        \n",
    "    def forward(self, x, y) -> Tuple[Tensor, Tensor]:\n",
    "        h = self.encoder(x)\n",
    "        hc = select(h, y)\n",
    "        x_hat = self.decoder(hc)\n",
    "        return h, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1024, 1, 1]), torch.Size([5, 3, 128, 128]), 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "S = 128\n",
    "x = torch.rand((N, 3, S, S))\n",
    "y = torch.randint(2, (N,))\n",
    "\n",
    "h, x_hat = model(x, y)\n",
    "\n",
    "_, C, S, _ = h.shape\n",
    "\n",
    "h.shape, x_hat.shape, C * S**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1., grad_fn=<DivBackward0>),\n",
       " tensor(0.4331, grad_fn=<L1LossBackward>),\n",
       " tensor(1.0433, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_l = act_loss(h, y)\n",
    "r_l = rec_loss(x, x_hat)\n",
    "loss = a_l + 0.1 * r_l\n",
    "\n",
    "a_l, r_l, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x: Tensor) -> Tensor:\n",
    "    return x\n",
    "\n",
    "\n",
    "def pool_gru(out_gru: Tensor) -> Tensor:\n",
    "    out, _ = out_gru\n",
    "    out_avg = torch.mean(out, dim=1)\n",
    "    out_max, _ = torch.max(out, dim=1)\n",
    "    return torch.cat([out_avg, out_max], dim=1)\n",
    "\n",
    "\n",
    "class FakeDetector(nn.Module):\n",
    "    def __init__(self, img_size: int, enc_dim: Tuple[int, int], \n",
    "                 seq_size: Tuple[int, int]):\n",
    "        super(FakeDetector, self).__init__()\n",
    "        self.autoenc = FakeDetector._build_encoder(img_size, enc_dim)\n",
    "        seq_in, seq_out = seq_size\n",
    "        self.pool = FakeDetector._build_pooling(img_size, enc_dim, seq_in)\n",
    "        self.seq_model = nn.GRU(seq_in, seq_out)\n",
    "        self.out = nn.Linear(seq_out*2, 1, bias=False)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_encoder(img_size: int, enc_dim: Tuple[int, int]) -> Autoencoder:\n",
    "        depth, size = enc_dim\n",
    "        if img_size % 32:\n",
    "            raise AttributeError('Image size should be a multiple of 32')  \n",
    "        return Autoencoder(in_ch=3, depth=depth, size=size, pad=1)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_pooling(img_size: int, enc_dim: Tuple[int, int], \n",
    "                       seq_in: int) -> Callable:\n",
    "        enc_depth, enc_size = enc_dim\n",
    "        size_factor = 2**(enc_depth-1)\n",
    "        if size_factor > img_size:\n",
    "            raise AttributeError('Encoder is too deep (%d) for spatial '\n",
    "                                 'dim (%d, %d)' % (enc_depth, img_size, img_size))\n",
    "        emb_S = img_size // size_factor\n",
    "        emb_C = enc_size * size_factor\n",
    "        \n",
    "        if emb_C * emb_S**2 == seq_in:\n",
    "            return identity\n",
    "        else:\n",
    "            out_S = math.sqrt(seq_in / emb_S / enc_size)\n",
    "            if math.modf(out_S)[0] > 0:\n",
    "                raise AttributeError('Sequence input size is incompatible '\n",
    "                                     'with encoder out dims')\n",
    "            else:\n",
    "                out_S = int(out_S)\n",
    "                in_dim = (emb_C, emb_S, emb_S)\n",
    "                out_dim = (emb_C, out_S, out_S)\n",
    "                print('Using avg pooling: {} -> {}'.format(in_dim, out_dim))\n",
    "                return nn.AdaptiveAvgPool3d(out_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: Tensor, y: Tensor):\n",
    "        N, N_fr, C, H, W = x.shape\n",
    "        hidden, xs_hat = [], []\n",
    "        \n",
    "        for f in range(N_fr):\n",
    "            h, x_hat = self.autoenc(x[:,f], y)\n",
    "            hidden.append(h[:,None])\n",
    "            xs_hat.append(x_hat[:,None])\n",
    "            \n",
    "        hidden = torch.cat(hidden, dim=1)\n",
    "        xs_hat = torch.cat(xs_hat, dim=1)\n",
    "        \n",
    "        seq = self.pool(hidden).reshape(N, N_fr, -1)\n",
    "        seq_out = self.seq_model(seq)\n",
    "        seq_out = pool_gru(seq_out)\n",
    "        y_hat = self.out(seq_out)\n",
    "        \n",
    "        return hidden, xs_hat, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using avg pooling: (128, 16, 16) -> (128, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "img_size = 256\n",
    "depth = 9\n",
    "model = FakeDetector(img_size=256, enc_dim=(5, 8), seq_size=(2048, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 256, 16, 16]),\n",
       " torch.Size([2, 5, 3, 256, 256]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, n_frames = 2, 5\n",
    "\n",
    "x = torch.rand((N, n_frames, 3, img_size, img_size))\n",
    "y = torch.randint(2, (N,))\n",
    "\n",
    "h, x_hat, y_hat = model(x, y)\n",
    "\n",
    "h.shape, x_hat.shape, y_hat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
