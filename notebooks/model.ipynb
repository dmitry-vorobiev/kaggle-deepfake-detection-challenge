{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import partial\n",
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "from torchvision import transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.zoo.common import AutoEncoder, encoder_block, decoder_block\n",
    "from model.layers import conv3D, Lambda\n",
    "from model.ops import act, identity, select, pool_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tensor([\n",
    "    [1,1,0,  1,1,0],\n",
    "    [0,0,0,  0,0,0],\n",
    "    [1,0,1,  0,1,0],\n",
    "    [1,1,1,  0,0,0],\n",
    "    [0,0,0,  1,1,1]\n",
    "], dtype=torch.float32, requires_grad=True)[:,:,None,None]\n",
    "\n",
    "y = torch.tensor([1, 0, 1, 0, 1])\n",
    "\n",
    "all_neg = torch.zeros(y.size(0), dtype=torch.int64)\n",
    "all_pos = torch.ones(y.size(0), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, all_pos).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 0.0000, 0.3333, 0.0000, 1.0000], grad_fn=<ClampMaxBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, all_neg).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 0.0000, 0.6667, 1.0000, 0.0000], grad_fn=<ClampMaxBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, all_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(select(h, all_neg) + select(h, all_pos) == h).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, y).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 0.0000, 0.3333, 1.0000, 1.0000], grad_fn=<ClampMaxBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(select(h, y) + select(h, (1-y)) == h).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{ACT} =\n",
    "\\sum_{x ∈ S_0}\n",
    "|a_0(x) − 1| + |a_1(x)| +\n",
    "\\sum_{x ∈ S_1}\n",
    "|a_1(x) − 1| + |a_0(x)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(n: int) -> Tensor:\n",
    "    return torch.zeros(n, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def ones(n: int) -> Tensor:\n",
    "    return torch.ones(n, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def act_loss(x: Tensor, y: Tensor) -> Tensor:\n",
    "    pos = y.nonzero().reshape(-1)\n",
    "    neg = (y - 1).nonzero().reshape(-1)\n",
    "    x0, x1 = x[neg], x[pos]\n",
    "    n0, n1 = x0.size(0), x1.size(0)\n",
    "    \n",
    "    a0_x0 = act(x0, zeros(n0))\n",
    "    a1_x0 = act(x0, ones(n0))\n",
    "    \n",
    "    a1_x1 = act(x1, ones(n1))\n",
    "    a0_x1 = act(x1, zeros(n1))\n",
    "    \n",
    "    neg_loss = (a0_x0 - 1).abs() + a1_x0\n",
    "    pos_loss = (a1_x1 - 1).abs() + a0_x1\n",
    "\n",
    "    return (neg_loss.sum() + pos_loss.sum()) / y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_loss(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorOut = Tuple[FloatTensor, FloatTensor, FloatTensor]\n",
    "\n",
    "\n",
    "def middle_block(in_ch: int, out_ch: int, kernel=3, stride=2, bn=True) -> nn.Module:\n",
    "    conv = conv3D(in_ch, out_ch, kernel=kernel, stride=stride, bias=not bn)\n",
    "    relu = nn.ReLU(inplace=True)\n",
    "    layers = [conv, relu]\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm3d(out_ch))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDetector(nn.Module):\n",
    "    def __init__(self, img_size: int, enc_depth: int, enc_width: int,\n",
    "                 mid_layers: List[int], out_ch: int,\n",
    "                 pool_size: Tuple[int, int] = None):\n",
    "        super(FakeDetector, self).__init__()\n",
    "        if img_size % 32:\n",
    "            raise AttributeError(\"img_size should be a multiple of 32\")\n",
    "        if out_ch % 2:\n",
    "            raise AttributeError(\"out_ch should be an even number\")\n",
    "\n",
    "        size_factor = 2 ** (enc_depth - 1)\n",
    "        if size_factor > img_size:\n",
    "            raise AttributeError(\n",
    "                'Encoder dims (%d, %d) are incompatible with image '\n",
    "                'size (%d, %d)' % (enc_depth, enc_width, img_size, img_size))\n",
    "        emb_size = img_size // size_factor\n",
    "        emb_ch = enc_width * size_factor\n",
    "\n",
    "        self.encoder = AutoEncoder(in_ch=3, depth=enc_depth, width=enc_width)\n",
    "\n",
    "        if img_size // 2 ** (enc_depth - 1) == 1:\n",
    "            self.middle = Lambda(identity)\n",
    "            rnn_in = emb_ch\n",
    "\n",
    "        elif len(mid_layers) > 0:\n",
    "            n_mid = len(mid_layers)\n",
    "            mid_layers = [emb_ch] + mid_layers\n",
    "            out_size = emb_size // 2 ** n_mid\n",
    "            if not out_size:\n",
    "                raise AssertionError('Too many middle layers...')\n",
    "            layers = [middle_block(mid_layers[i], mid_layers[i + 1], stride=2)\n",
    "                      for i in range(n_mid)]\n",
    "            self.middle = nn.Sequential(*layers)\n",
    "            rnn_in = mid_layers[-1] * out_size ** 2\n",
    "\n",
    "        elif pool_size is not None:\n",
    "            D, H = pool_size\n",
    "            self.middle = nn.AdaptiveAvgPool3d((D, H, H))\n",
    "            rnn_in = emb_ch * H ** 2\n",
    "\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                'Both mid_layers and pool_size are missing. '\n",
    "                'Unable to build model with provided configuration')\n",
    "\n",
    "        self.rnn = nn.GRU(rnn_in, out_ch // 2)\n",
    "        self.out = nn.Linear(out_ch, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: FloatTensor, y: LongTensor) -> DetectorOut:\n",
    "        N, C, D, H, W = x.shape\n",
    "        hidden, xs_hat = [], []\n",
    "\n",
    "        for f in range(D):\n",
    "            h, x_hat = self.encoder(x[:, :, f], y)\n",
    "            hidden.append(h.unsqueeze(2))\n",
    "            xs_hat.append(x_hat.unsqueeze(2))\n",
    "\n",
    "        hidden = torch.cat(hidden, dim=2)\n",
    "        xs_hat = torch.cat(xs_hat, dim=2)\n",
    "\n",
    "        seq = self.middle(hidden).reshape(N, D, -1)\n",
    "        seq_out = self.rnn(seq)\n",
    "        seq_out = pool_gru(seq_out)\n",
    "        y_hat = self.out(seq_out)\n",
    "\n",
    "        return hidden, xs_hat, y_hat, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_size = 256\n",
    "\n",
    "# model = FakeDetector(\n",
    "#     img_size=img_size, \n",
    "#     enc_depth=5, \n",
    "#     enc_width=8, \n",
    "#     mid_layers=[128, 128],\n",
    "#     out_ch=128\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, n_frames = 16, 10\n",
    "\n",
    "# x = torch.randn((N, 3, n_frames, img_size, img_size), device=device)\n",
    "# y = torch.randint(2, (N,), device=device)\n",
    "\n",
    "# h, x_hat, y_hat, seq = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.modules():\n",
    "#     for cls in [nn.Conv2d, nn.Conv3d, nn.Linear]:\n",
    "#         if isinstance(layer, cls):\n",
    "#             nn.init.kaiming_uniform_(layer.weight, a=0)\n",
    "#             if layer.bias is not None:\n",
    "#                 layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var, name in zip([x, h, x_hat, y_hat, seq], ['x', 'h', 'x_hat', 'y_hat', 'seq']):\n",
    "#     mean, std = var.mean().item(), var.std().item()\n",
    "#     shape = ', '.join(map(str, var.shape))\n",
    "#     print('{:5s} | mean {: .03f}, std {:.03f} | ({})'.format(name, mean, std, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layers import conv2D\n",
    "from model.ops import select, pool_gru\n",
    "\n",
    "\n",
    "class ShrinkFork(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, fork_depth: int):\n",
    "        super(ShrinkFork, self).__init__()\n",
    "        self.main = encoder_block(in_ch, out_ch)\n",
    "        aux = [encoder_block(out_ch * 2**p, out_ch * 2**(p+1), stride=2)\n",
    "               for p in range(fork_depth)]\n",
    "        self.fork = nn.Sequential(*aux)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        c = self.fork(x)\n",
    "        return x, c\n",
    "\n",
    "\n",
    "class Bilbo(nn.Module):\n",
    "    def __init__(self, image_shape: Tuple[int, int, int], enc_depth: int, \n",
    "                 enc_width: int, fork_depth: int, rnn_width: int):\n",
    "        super(Bilbo, self).__init__()\n",
    "        C, H, W = image_shape\n",
    "        if H != W:\n",
    "            raise AttributeError(\"Only square images are supported!\")\n",
    "        max_fork_depth = math.log2(H) - 1 \n",
    "        if fork_depth > max_fork_depth:\n",
    "            raise AttributeError(\n",
    "                \"fork_depth should be <= {} given the image_size \"\n",
    "                \" ({}, {})\".format(int(max_fork_depth), H, H))\n",
    "        \n",
    "        self.stem = encoder_block(C, enc_width, stride=1, bn=False)\n",
    "        encoder_layers = [ShrinkFork(enc_width * 2**i, enc_width * 2**(i+1), \n",
    "                                     fork_depth=(fork_depth-i))\n",
    "                          for i in range(0, enc_depth - 1)]\n",
    "        self.encoder = nn.ModuleList(encoder_layers)\n",
    "        \n",
    "        decoder_layers = [decoder_block(enc_width * 2**(i+1), enc_width * 2**i)\n",
    "                          for i in sorted(range(0, enc_depth - 1), reverse=True)]\n",
    "        last = conv2D(enc_width, C, kernel=3, stride=1)\n",
    "        self.decoder = nn.Sequential(*decoder_layers, last, nn.Tanh())\n",
    "        \n",
    "        rnn_in = enc_width * (H * W / 4) / 2**(fork_depth - 1) * (enc_depth - 1)\n",
    "        assert not math.modf(rnn_in)[0]\n",
    "        rnn_in = int(rnn_in)\n",
    "        self.gru = nn.GRU(rnn_in, rnn_width, bidirectional=True)\n",
    "        self.out = nn.Linear(rnn_width * 4, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: FloatTensor, y: LongTensor):\n",
    "        N, C, D, H, W = x.shape\n",
    "        hidden, x_rec, features = [], [], []\n",
    "        \n",
    "        for f in range(D):\n",
    "            cc = []\n",
    "            h = self.stem(x[:, :, f])\n",
    "            for i in range(len(self.encoder)):\n",
    "                h, c = self.encoder[i](h)\n",
    "                cc.append(c)\n",
    "            hidden.append(h.unsqueeze(2))\n",
    "            features.append(torch.cat(cc, dim=1).unsqueeze(2))\n",
    "            \n",
    "            hc = select(h, y)\n",
    "            x1 = self.decoder(hc)\n",
    "            x_rec.append(x1.unsqueeze(2))\n",
    "            \n",
    "        hidden = torch.cat(hidden, dim=2)\n",
    "        x_rec = torch.cat(x_rec, dim=2)\n",
    "        features = torch.cat(features, dim=2)\n",
    "        \n",
    "        gru_out = self.gru(features.reshape(N, D, -1))\n",
    "        gru_out = pool_gru(gru_out)\n",
    "        \n",
    "        y_hat = self.out(gru_out)\n",
    "        return hidden, x_rec, y_hat, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "deivce = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, H, W = 2, 5, 128, 128\n",
    "\n",
    "x = torch.randn((N, 3, D, H, W)).to(device)\n",
    "y = torch.randint(2, (N,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilbo = Bilbo(\n",
    "    image_shape=(3, H, W), \n",
    "    enc_depth=5, \n",
    "    enc_width=8, \n",
    "    fork_depth=5,\n",
    "    rnn_width=64\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 5, 8, 8])\n",
      "torch.Size([2, 3, 5, 128, 128])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 2048, 5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "out = bilbo(x, y)\n",
    "\n",
    "for e in out:\n",
    "    print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x     | mean  0.001, std 1.000 | (2, 3, 5, 128, 128)\n",
      "h     | mean -0.000, std 1.000 | (2, 128, 5, 8, 8)\n",
      "x_hat | mean -0.006, std 0.668 | (2, 3, 5, 128, 128)\n",
      "y_hat | mean  0.325, std 0.180 | (2, 1)\n",
      "feat  | mean -0.000, std 0.999 | (2, 2048, 5, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "for var, name in zip([x] + list(out), ['x', 'h', 'x_hat', 'y_hat', 'feat']):\n",
    "    mean, std = var.mean().item(), var.std().item()\n",
    "    shape = ', '.join(map(str, var.shape))\n",
    "    print('{:5s} | mean {: .03f}, std {:.03f} | ({})'.format(name, mean, std, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frodo(nn.Module):\n",
    "    def __init__(self, image_shape: Tuple[int, int, int], enc_depth: int, enc_width: int):\n",
    "        super(Frodo, self).__init__()\n",
    "        C, H, W = image_shape\n",
    "        if H != W:\n",
    "            raise AttributeError(\"Only square images are supported!\")\n",
    "            \n",
    "        max_depth = math.log2(H) + 1 \n",
    "        if enc_depth > max_depth:\n",
    "            raise AttributeError(\n",
    "                \"enc_depth should be <= {} given the image_size \"\n",
    "                \" ({}, {})\".format(int(max_depth), H, H))\n",
    "        \n",
    "        stem = encoder_block(C, enc_width, stride=1, bn=False)\n",
    "        encoder_layers = [encoder_block(enc_width * 2**i, enc_width * 2**(i+1))\n",
    "                          for i in range(0, enc_depth - 1)]\n",
    "        self.encoder = nn.Sequential(stem, *encoder_layers)\n",
    "        \n",
    "        decoder_layers = [decoder_block(enc_width * 2**(i+1), enc_width * 2**i)\n",
    "                          for i in sorted(range(0, enc_depth - 1), reverse=True)]\n",
    "        last = conv2D(enc_width, C, kernel=3, stride=1)\n",
    "        self.decoder = nn.Sequential(*decoder_layers, last, nn.Tanh())\n",
    "\n",
    "    def forward(self, x: FloatTensor, y: LongTensor):\n",
    "        N, C, D, H, W = x.shape\n",
    "        hidden, x_rec = [], []\n",
    "        \n",
    "        for f in range(D):\n",
    "            h = self.encoder(x[:, :, f])\n",
    "            hc = select(h, y)\n",
    "            x1 = self.decoder(hc)\n",
    "            \n",
    "            hidden.append(h.unsqueeze(2))\n",
    "            x_rec.append(x1.unsqueeze(2))\n",
    "            \n",
    "        hidden = torch.cat(hidden, dim=2)\n",
    "        x_rec = torch.cat(x_rec, dim=2)\n",
    "        return hidden, x_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "frodo = Frodo(\n",
    "    image_shape=(3, H, W), \n",
    "    enc_depth=5, \n",
    "    enc_width=8,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = frodo(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x     | mean  0.001, std 1.000 | (2, 3, 5, 128, 128)\n",
      "h     | mean  0.000, std 1.000 | (2, 128, 5, 8, 8)\n",
      "x_hat | mean -0.020, std 0.622 | (2, 3, 5, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "for var, name in zip([x] + list(out), ['x', 'h', 'x_hat']):\n",
    "    mean, std = var.mean().item(), var.std().item()\n",
    "    shape = ', '.join(map(str, var.shape))\n",
    "    print('{:5s} | mean {: .03f}, std {:.03f} | ({})'.format(name, mean, std, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(n: int, device: torch.device) -> Tensor:\n",
    "    return torch.zeros(n, dtype=torch.int64, device=device)\n",
    "\n",
    "\n",
    "def ones(n: int, device: torch.device) -> Tensor:\n",
    "    return torch.ones(n, dtype=torch.int64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ops import reshape_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_5dims(h: Tensor, y: Tensor, average=False) -> Tensor:\n",
    "    N, C, D, H, W = h.shape\n",
    "    y = reshape_as(y, h)\n",
    "    h0, h1 = h.chunk(2, dim=1)\n",
    "    a = h0 * (1 - y) + h1 * y\n",
    "\n",
    "    if average:\n",
    "        n_el = a.numel() / max(N, 1)\n",
    "        a = a.abs().sum(tuple(range(1, a.ndim))) / n_el\n",
    "    else:\n",
    "        n_el = a.numel() / max(N * D, 1)\n",
    "        a = a.abs().sum((1, 3, 4)) / n_el\n",
    "\n",
    "    # For simplicity, and without losing generality, \n",
    "    # we constrain a(x) to be equal to 1\n",
    "    return a.clamp_max_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 128, 128])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide(h: Tensor, device: torch.device) -> Tensor:\n",
    "    N = h.size(0)\n",
    "    a0 = act_5dims(h, zeros(N, device), average=False).unsqueeze(2)\n",
    "    a1 = act_5dims(h, ones(N, device), average=False).unsqueeze(2)\n",
    "    a = torch.cat([a0, a1], dim=2)\n",
    "    _, y_pred = torch.max(a, dim=2)\n",
    "    return y_pred.float().mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000], device='cuda:1')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, x_rec = frodo(x, y)\n",
    "\n",
    "decide(h, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
