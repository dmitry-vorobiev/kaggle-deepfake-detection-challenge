{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import partial\n",
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "from torchvision import transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/dmitry/projects/dfdc'\n",
    "SRC_DIR = os.path.join(BASE_DIR, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.autoencoder import AutoEncoder\n",
    "from model.layers import conv3D, Lambda\n",
    "from model.ops import act, identity, select, pool_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tensor([\n",
    "    [1,1,0,  1,1,0],\n",
    "    [0,0,0,  0,0,0],\n",
    "    [1,0,1,  0,1,0],\n",
    "    [1,1,1,  0,0,0],\n",
    "    [0,0,0,  1,1,1]\n",
    "], dtype=torch.float32, requires_grad=True)[:,:,None,None]\n",
    "\n",
    "y = torch.tensor([1, 0, 1, 0, 1])\n",
    "\n",
    "all_neg = torch.zeros(y.size(0), dtype=torch.int64)\n",
    "all_pos = torch.ones(y.size(0), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, all_pos).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 0., 1.], grad_fn=<CeilBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, all_neg).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 0.], grad_fn=<CeilBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, all_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(select(h, all_neg) + select(h, all_pos) == h).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(h, y).reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 1.], grad_fn=<CeilBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(select(h, y) + select(h, (1-y)) == h).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{ACT} =\n",
    "\\sum_{x ∈ S_0}\n",
    "|a_0(x) − 1| + |a_1(x)| +\n",
    "\\sum_{x ∈ S_1}\n",
    "|a_1(x) − 1| + |a_0(x)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(n: int) -> Tensor:\n",
    "    return torch.zeros(n, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def ones(n: int) -> Tensor:\n",
    "    return torch.ones(n, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def act_loss(x: Tensor, y: Tensor) -> Tensor:\n",
    "    pos = y.nonzero().reshape(-1)\n",
    "    neg = (y - 1).nonzero().reshape(-1)\n",
    "    x0, x1 = x[neg], x[pos]\n",
    "    n0, n1 = x0.size(0), x1.size(0)\n",
    "    \n",
    "    a0_x0 = act(x0, zeros(n0))\n",
    "    a1_x0 = act(x0, ones(n0))\n",
    "    \n",
    "    a1_x1 = act(x1, ones(n1))\n",
    "    a0_x1 = act(x1, zeros(n1))\n",
    "    \n",
    "    neg_loss = (a0_x0 - 1).abs() + a1_x0\n",
    "    pos_loss = (a1_x1 - 1).abs() + a0_x1\n",
    "\n",
    "    return (neg_loss.sum() + pos_loss.sum()) / y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6000, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_loss(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorOut = Tuple[FloatTensor, FloatTensor, FloatTensor]\n",
    "\n",
    "\n",
    "def middle_block(in_ch: int, out_ch: int, kernel=3, stride=2, bn=True) -> nn.Module:\n",
    "    conv = conv3D(in_ch, out_ch, kernel=kernel, stride=stride, bias=not bn)\n",
    "    relu = nn.ReLU(inplace=True)\n",
    "    layers = [conv, relu]\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm3d(out_ch))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDetector(nn.Module):\n",
    "    def __init__(self, img_size: int, enc_depth: int, enc_width: int,\n",
    "                 mid_layers: List[int], out_ch: int,\n",
    "                 pool_size: Tuple[int, int] = None):\n",
    "        super(FakeDetector, self).__init__()\n",
    "        if img_size % 32:\n",
    "            raise AttributeError(\"img_size should be a multiple of 32\")\n",
    "        if out_ch % 2:\n",
    "            raise AttributeError(\"out_ch should be an even number\")\n",
    "\n",
    "        size_factor = 2 ** (enc_depth - 1)\n",
    "        if size_factor > img_size:\n",
    "            raise AttributeError(\n",
    "                'Encoder dims (%d, %d) are incompatible with image '\n",
    "                'size (%d, %d)' % (enc_depth, enc_width, img_size, img_size))\n",
    "        emb_size = img_size // size_factor\n",
    "        emb_ch = enc_width * size_factor\n",
    "\n",
    "        self.encoder = AutoEncoder(in_ch=3, depth=enc_depth, width=enc_width)\n",
    "\n",
    "        if img_size // 2 ** (enc_depth - 1) == 1:\n",
    "            self.middle = Lambda(identity)\n",
    "            rnn_in = emb_ch\n",
    "\n",
    "        elif len(mid_layers) > 0:\n",
    "            n_mid = len(mid_layers)\n",
    "            mid_layers = [emb_ch] + mid_layers\n",
    "            out_size = emb_size // 2 ** n_mid\n",
    "            if not out_size:\n",
    "                raise AssertionError('Too many middle layers...')\n",
    "            layers = [middle_block(mid_layers[i], mid_layers[i + 1], stride=2)\n",
    "                      for i in range(n_mid)]\n",
    "            self.middle = nn.Sequential(*layers)\n",
    "            rnn_in = mid_layers[-1] * out_size ** 2\n",
    "\n",
    "        elif pool_size is not None:\n",
    "            D, H = pool_size\n",
    "            self.middle = nn.AdaptiveAvgPool3d((D, H, H))\n",
    "            rnn_in = emb_ch * H ** 2\n",
    "\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                'Both mid_layers and pool_size are missing. '\n",
    "                'Unable to build model with provided configuration')\n",
    "\n",
    "        self.rnn = nn.GRU(rnn_in, out_ch // 2)\n",
    "        self.out = nn.Linear(out_ch, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: FloatTensor, y: LongTensor) -> DetectorOut:\n",
    "        N, C, D, H, W = x.shape\n",
    "        hidden, xs_hat = [], []\n",
    "\n",
    "        for f in range(D):\n",
    "            h, x_hat = self.encoder(x[:, :, f], y)\n",
    "            hidden.append(h.unsqueeze(2))\n",
    "            xs_hat.append(x_hat.unsqueeze(2))\n",
    "\n",
    "        hidden = torch.cat(hidden, dim=2)\n",
    "        xs_hat = torch.cat(xs_hat, dim=2)\n",
    "\n",
    "        seq = self.middle(hidden).reshape(N, D, -1)\n",
    "        seq_out = self.rnn(seq)\n",
    "        seq_out = pool_gru(seq_out)\n",
    "        y_hat = self.out(seq_out)\n",
    "\n",
    "        return hidden, xs_hat, y_hat, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "\n",
    "model = FakeDetector(\n",
    "    img_size=img_size, \n",
    "    enc_depth=5, \n",
    "    enc_width=8, \n",
    "    mid_layers=[128, 128],\n",
    "    out_ch=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, n_frames = 16, 10\n",
    "\n",
    "x = torch.randn((N, 3, n_frames, img_size, img_size), device=device)\n",
    "y = torch.randint(2, (N,), device=device)\n",
    "\n",
    "h, x_hat, y_hat, seq = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.modules():\n",
    "#     for cls in [nn.Conv2d, nn.Conv3d, nn.Linear]:\n",
    "#         if isinstance(layer, cls):\n",
    "#             nn.init.kaiming_uniform_(layer.weight, a=0)\n",
    "#             if layer.bias is not None:\n",
    "#                 layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x     | mean  0.000, std 1.000 | (16, 3, 10, 256, 256)\n",
      "h     | mean -0.000, std 1.000 | (16, 128, 10, 16, 16)\n",
      "x_hat | mean -0.031, std 0.654 | (16, 3, 10, 256, 256)\n",
      "y_hat | mean  0.708, std 0.052 | (16, 1)\n",
      "seq   | mean -0.000, std 1.000 | (16, 10, 2048)\n"
     ]
    }
   ],
   "source": [
    "for var, name in zip([x, h, x_hat, y_hat, seq], ['x', 'h', 'x_hat', 'y_hat', 'seq']):\n",
    "    mean, std = var.mean().item(), var.std().item()\n",
    "    shape = ', '.join(map(str, var.shape))\n",
    "    print('{:5s} | mean {: .03f}, std {:.03f} | ({})'.format(name, mean, std, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_2 = nn.Sequential(\n",
    "#     nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "#     # nn.BatchNorm2d(8),\n",
    "#     nn.ReLU(inplace=True)\n",
    "# ).to(device)\n",
    "\n",
    "# conv_3 = nn.Sequential(\n",
    "#     conv3D(3, 8, kernel=3, stride=1, pad=1),\n",
    "#     nn.ReLU(inplace=True)\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in [conv_2, conv_3]:\n",
    "#     for layer in model:\n",
    "#         for cls in [nn.Conv2d, nn.Conv3d, nn.Linear]:\n",
    "#             if isinstance(layer, cls):\n",
    "#                 nn.init.kaiming_normal_(layer.weight, a=0)\n",
    "#                 if layer.bias is not None:\n",
    "#                     layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2 = conv_2(x[:, :, 0])\n",
    "# x3 = conv_3(x)\n",
    "\n",
    "# for var, name in zip([x2, x3], ['x2', 'x3']):\n",
    "#     mean, std = var.mean().item(), var.std().item()\n",
    "#     shape = ', '.join(map(str, var.shape))\n",
    "#     print('{:5s}: mean {:.03f}, std {:.03f}, ({})'.format(name, mean, std, shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
